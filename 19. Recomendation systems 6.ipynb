{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "204e97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Создаем spark сессию\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sql_func\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config('spark.driver.memory', '12G')\n",
    "    .config('spark.sql.analyzer.failAmbiguousSelfJoin', 'False')\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27e02a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'D:/Datasets/ml-latest-small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e8515cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(DATA_DIR + 'ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7abac0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "462cc5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала посмотрим на общее распределение тегов\n",
    "ratings = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        os.path.join(DATA_DIR, 'ratings.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b26bbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала посмотрим на общее распределение тегов\n",
    "raw_tags = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        os.path.join(DATA_DIR, 'tags.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb7a3a1a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "movies = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        os.path.join(DATA_DIR, 'movies.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    # если используется меньше памяти,\n",
    "    # то здесь можно взять не все данные, а небольшую выборку\n",
    "    # даже при fraction=.01 качественная картина не меняеся\n",
    "    .withColumn('genres_list', sql_func.split('genres', '\\|'))\n",
    "    .select('movieId', 'title', 'genres_list')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8c6e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# нас ек будет интересовать, какой именно пользователь поставил тег и когда это произошло\n",
    "tags = (\n",
    "    raw_tags\n",
    "    # теги могут различатьтся только регистром\n",
    "    # поэтому приведём их все к верхнему\n",
    "    .select(\n",
    "        sql_func.col(\"movieId\"),\n",
    "        sql_func.upper(sql_func.col(\"tag\")).alias(\"tag\")\n",
    "    )\n",
    "    .groupBy(\"movieId\")\n",
    "    .agg(sql_func.collect_list(\"tag\").alias(\"tags_list\"))\n",
    "    .join(movies, \"movieId\", \"right\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94d0443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединим теги и жанры в единое пространство текстовых фич\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# В Spark нет некоторых полезных функций, но легко можно создать свои (UDF - user defined function)\n",
    "#  в часности, мы хотим провести все жанры также к верхнему регистру\n",
    "list_concat = sql_func.udf(\n",
    "    lambda one_list, another_list:\n",
    "        [str.upper(elem) for elem in one_list] + (another_list if another_list else []),\n",
    "    returnType=ArrayType(StringType())\n",
    ")\n",
    "\n",
    "content_features = (\n",
    "    tags\n",
    "    .select(\n",
    "        \"movieId\",\n",
    "        \"title\",\n",
    "        list_concat(\"genres_list\", \"tags_list\").alias(\"content_features\")\n",
    "    )\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5167513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посчитаем частоты встречаемости для тегов для всех фильмов\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "term_frequencies = HashingTF(\n",
    "    # от каждого тега будет вычислен хэш\n",
    "    # и по факту мы будем считать частоты бакетов хэшей, а не для самих тэгов\n",
    "    numFeatures=1024,\n",
    "    inputCol=\"content_features\",\n",
    "    outputCol=\"term_frequencies\"\n",
    ").transform(content_features).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19057650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь сделаем поправку на частоту тегов в целом, чтобы убрать неинформативные теги\n",
    "# это второй шаг TF-IDF (term frequency, inverted document frequency)\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf_model = IDF(\n",
    "    inputCol=\"term_frequencies\",\n",
    "    outputCol=\"tf_idf\",\n",
    "    minDocFreq=2\n",
    ").fit(term_frequencies)\n",
    "tf_idf = (\n",
    "    idf_model\n",
    "    .transform(term_frequencies)\n",
    "    .select(\"movieId\", \"title\", \"tf_idf\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57259169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "import numpy as np\n",
    "from pyspark.sql.types import FloatType, ArrayType\n",
    "\n",
    "def sklearn_lr(spark_x: list, spark_y: list) -> list:\n",
    "    \"\"\"\n",
    "        spark_x: список pyspark.ml.linalg,SparseVector - фичи для регрессии\n",
    "        spark_y: список занчений целевой переменной регрессии\n",
    "        return: список коэффициентов регресии\n",
    "    \"\"\"\n",
    "    # переводим данные из формата spark  в формат, удобный sklearn\n",
    "    numpy_x = np.array([vector.toArray() for vector in spark_x])\n",
    "    numpy_y = np.array(spark_y).reshape(-1, 1)\n",
    "    # применяем обычную модель из sklearn\n",
    "    lr = ElasticNet().fit(numpy_x, numpy_y)\n",
    "    # возвращаем в ответ плотный вектор коэффициентов регрессии\n",
    "    return (lr.sparse_coef_.todense().tolist()[0], lr.intercept_.tolist())\n",
    "\n",
    "# определяем Spark UDF, которая обучит регрессия на своих аргементах\n",
    "reg_udf = sql_func.udf(\n",
    "    sklearn_lr,\n",
    "    returnType=ArrayType(ArrayType(FloatType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "375ab36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# рабираем полученые данные на обучающую и тестовую выборки\n",
    "train_data, test_data = ratings.join(tf_idf, \"movieId\").randomSplit([.8, .2], seed=42)\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "# строим для каждого пользователя свою модель регессии\n",
    "model_coef = (\n",
    "    train_data\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(\n",
    "        sql_func.collect_list(\"tf_idf\").alias(\"x\"),\n",
    "        sql_func.collect_list(\"rating\").alias(\"y\")\n",
    "    )\n",
    "    .withColumn(\"model_coeff\", reg_udf(\"x\", \"y\"))\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c87d36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# на всякий случай сохраним полученные коэффициенты на диск\n",
    "model_coef.coalesce(1).write.mode(\"overwrite\").parquet(os.path.join(DATA_DIR, \"model_coef.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17e835f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_coef.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b06ec351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+\n",
      "|userId|                   x|                   y|         model_coeff|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "|   148|[(1024,[11,88,113...|[4.0, 4.0, 4.0, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|   463|[(1024,[89,143,14...|[4.5, 4.0, 4.0, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|   471|[(1024,[156,273,4...|[5.0, 3.0, 4.5, 5...|[[0.0, 0.0, 0.0, ...|\n",
      "|   496|[(1024,[409,668,7...|[1.0, 5.0, 5.0, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|   243|[(1024,[263,287,6...|[5.0, 5.0, 4.0, 5...|[[0.0, 0.0, 0.0, ...|\n",
      "|   392|[(1024,[45,219,75...|[3.0, 1.0, 2.0, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|   540|[(1024,[219,263,2...|[3.5, 5.0, 4.5, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|    31|[(1024,[836,837,8...|[3.0, 4.0, 4.0, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|   516|[(1024,[15,143,14...|[5.0, 4.0, 4.0, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|    85|[(1024,[219,668],...|[5.0, 5.0, 5.0, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|   137|[(1024,[89,143,14...|[4.0, 5.0, 3.5, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|   251|[(1024,[263,307,4...|[5.0, 5.0, 5.0, 5...|[[0.0, 0.0, 0.0, ...|\n",
      "|   451|[(1024,[156,273,4...|[5.0, 3.0, 4.0, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|   580|[(1024,[156,273,4...|[3.0, 4.0, 3.5, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|    65|[(1024,[65,219,38...|[4.0, 4.0, 4.5, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|   458|[(1024,[204,428,5...|[4.0, 3.0, 5.0, 5...|[[0.0, 0.0, 0.0, ...|\n",
      "|    53|[(1024,[867],[0.9...|[5.0, 5.0, 5.0, 5...|[[0.0, 0.0, 0.0, ...|\n",
      "|   255|[(1024,[219,438,6...|[1.0, 2.0, 5.0, 2...|[[0.0, 0.0, 0.0, ...|\n",
      "|   481|[(1024,[605,668,9...|[3.0, 4.0, 2.0, 2...|[[0.0, 0.0, 0.0, ...|\n",
      "|   588|[(1024,[449,759,8...|[3.0, 5.0, 3.0, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_coef.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "514ce682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "\n",
    "def lr_apply(x: SparseVector, lr_coef: list) -> float:\n",
    "    \"\"\"\n",
    "        param x: векторр фич для регрессии\n",
    "        param lr_coef: \n",
    "        return предсказанное моделью регруссии значение\n",
    "    \"\"\"\n",
    "    return float(np.array(x).dot(np.array(lr_coef[0])) + lr_coef[1][0])\n",
    "\n",
    "lr_apply_udf = sql_func.udf(lr_apply, returnType=ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "535098d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def get_prediction(data: DataFrame) -> float:\n",
    "    return(\n",
    "        data\n",
    "        .join(model_coef, \"userId\")\n",
    "        .select(\n",
    "            \"userId\",\n",
    "            \"rating\",\n",
    "            \"movieId\",\n",
    "            \"tf_idf\",\n",
    "            lr_apply_udf(\"tf_idf\", \"model_coeff\").alias(\"prediction\"))\n",
    "        .cache()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a992ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction = get_prediction(train_data)\n",
    "(\n",
    "    train_prediction.write.mode(\"overwrite\")\n",
    "    .parquet(os.path.join(DATA_DIR, \"train_prediction.parquet\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91705750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df322b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(prediction: DataFrame) -> float:\n",
    "    return np.sqrt(\n",
    "        prediction\n",
    "        .selectExpr(\"\"\"\n",
    "        CASE\n",
    "            WHEN prediction > 5 THEN 5\n",
    "            WHEN prediction < .5 THEN .5\n",
    "            ELSE prediction\n",
    "        END AS prediction\n",
    "        \"\"\", \"rating\")\n",
    "        .select(\n",
    "            sql_func.pow(sql_func.col(\"rating\") - sql_func.col(\"prediction\"), 2)\n",
    "            .alias(\"squared error\")\n",
    "        )\n",
    "        .agg(sql_func.avg(\"squared_error\"))\n",
    "        .first()[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95221ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prediction(train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = get_prediction(test_data)\n",
    "(\n",
    "    test_prediction.write.mode(\"overwrite\")\n",
    "    .parquet(os.path.join(DATA_DIR, \"test_prediction.parquet\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prediction(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763109ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.where(\"title LIKE 'Top gun%'\").select(\"title\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71af42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
