{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204e97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Создаем spark сессию\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sql_func\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config('spark.driver.memory', '16G')\n",
    "    .config('spark.sql.analyzer.failAmbiguousSelfJoin', 'False')\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e02a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'D:/Datasets/ml-latest-small/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8515cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(DATA_DIR + 'ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7abac0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "462cc5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала посмотрим на общее распределение тегов\n",
    "ratings = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        os.path.join(DATA_DIR, 'ratings.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b26bbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала посмотрим на общее распределение тегов\n",
    "raw_tags = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        os.path.join(DATA_DIR, 'tags.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb7a3a1a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "movies = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        os.path.join(DATA_DIR, 'movies.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    # если используется меньше памяти,\n",
    "    # то здесь можно взять не все данные, а небольшую выборку\n",
    "    # даже при fraction=.01 качественная картина не меняеся\n",
    "    .withColumn('genres_list', sql_func.split('genres', '\\|'))\n",
    "    .select('movieId', 'title', 'genres_list')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c6e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# нас ек будет интересовать, какой именно пользователь поставил тег и когда это произошло\n",
    "tags = (\n",
    "    raw_tags\n",
    "    # теги могут различатьтся только регистром\n",
    "    # поэтому приведём их все к верхнему\n",
    "    .select(\n",
    "        sql_func.col(\"movieId\"),\n",
    "        sql_func.upper(sql_func.col(\"tag\")).alias(\"tag\")\n",
    "    )\n",
    "    .groupBy(\"movieId\")\n",
    "    .agg(sql_func.collect_list(\"tag\").alias(\"tags_list\"))\n",
    "    .join(movies, \"movieId\", \"right\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d0443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединим теги и жанры в единое пространство текстовых фич\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# В Spark нет некоторых полезных функций, но легко можно создать свои (UDF - user defined function)\n",
    "#  в часности, мы хотим провести все жанры также к верхнему регистру\n",
    "list_concat = sql_func.udf(\n",
    "    lambda one_list, another_list:\n",
    "        [str.upper(elem) for elem in one_list] + (another_list if another_list else []),\n",
    "    returnType=ArrayType(StringType())\n",
    ")\n",
    "\n",
    "content_features = (\n",
    "    tags\n",
    "    .select(\n",
    "        \"movieId\",\n",
    "        \"title\",\n",
    "        list_concat(\"genres_list\", \"tags_list\").alias(\"content_features\")\n",
    "    )\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5167513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посчитаем частоты встречаемости для тегов для всех фильмов\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "term_frequencies = HashingTF(\n",
    "    # от каждого тега будет вычислен хэш\n",
    "    # и по факту мы будем считать частоты бакетов хэшей, а не для самих тэгов\n",
    "    numFeatures=1024,\n",
    "    inputCol=\"content_features\",\n",
    "    outputCol=\"term_frequencies\"\n",
    ").transform(content_features).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19057650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь сделаем поправку на частоту тегов в целом, чтобы убрать неинформативные теги\n",
    "# это второй шаг TF-IDF (term frequency, inverted document frequency)\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf_model = IDF(\n",
    "    inputCol=\"term_frequencies\",\n",
    "    outputCol=\"tf_idf\",\n",
    "    minDocFreq=2\n",
    ").fit(term_frequencies)\n",
    "tf_idf = (\n",
    "    idf_model\n",
    "    .transform(term_frequencies)\n",
    "    .select(\"movieId\", \"title\", \"tf_idf\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57259169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "import numpy as np\n",
    "from pyspark.sql.types import FloatType, ArrayType\n",
    "\n",
    "def sklearn_lr(spark_x: list, spark_y: list) -> list:\n",
    "    \"\"\"\n",
    "        spark_x: список pyspark.ml.linalg,SparseVector - фичи для регрессии\n",
    "        spark_y: список занчений целевой переменной регрессии\n",
    "        return: список коэффициентов регресии\n",
    "    \"\"\"\n",
    "    # переводим данные из формата spark  в формат, удобный sklearn\n",
    "    numpy_x = np.array([vector.toArray() for vector in spark_x])\n",
    "    numpy_y = np.array(spark_y).reshape(-1, 1)\n",
    "    # применяем обычную модель из sklearn\n",
    "    lr = ElasticNet().fit(numpy_x, numpy_y)\n",
    "    # возвращаем в ответ плотный вектор коэффициентов регрессии\n",
    "    return (lr.sparse_coef_.todense().tolist()[0], lr.intercept_.tolist())\n",
    "\n",
    "# определяем Spark UDF, которая обучит регрессия на своих аргементах\n",
    "reg_udf = sql_func.udf(\n",
    "    sklearn_lr,\n",
    "    returnType=ArrayType(ArrayType(FloatType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "375ab36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# рабираем полученые данные на обучающую и тестовую выборки\n",
    "train_data, test_data = ratings.join(tf_idf, \"movieId\").randomSplit([.8, .2], seed=42)\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "# строим для каждого пользователя свою модель регессии\n",
    "model_coef = (\n",
    "    train_data\n",
    "    .groupBy(\"userId\")\n",
    "    .agg(\n",
    "        sql_func.collect_list(\"tf_idf\").alias(\"x\"),\n",
    "        sql_func.collect_list(\"rating\").alias(\"y\")\n",
    "    )\n",
    "    .withColumn(\"model_coef\", reg_udf(\"x\", \"y\"))\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c87d36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# на всякий случай сохраним полученные коэффициенты на диск\n",
    "model_coef.coalesce(1).write.mode(\"overwrite\").parquet(os.path.join(DATA_DIR, \"model_coef.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17e835f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_coef.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b06ec351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+\n",
      "|userId|                   x|                   y|          model_coef|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "|   148|[(1024,[11,88,113...|[4.0, 4.0, 4.0, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|   463|[(1024,[89,143,14...|[4.5, 4.0, 4.0, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|   471|[(1024,[156,273,4...|[5.0, 3.0, 4.5, 5...|[[0.0, 0.0, 0.0, ...|\n",
      "|   496|[(1024,[409,668,7...|[1.0, 5.0, 5.0, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|   243|[(1024,[263,287,6...|[5.0, 5.0, 4.0, 5...|[[0.0, 0.0, 0.0, ...|\n",
      "|   392|[(1024,[45,219,75...|[3.0, 1.0, 2.0, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|   540|[(1024,[219,263,2...|[3.5, 5.0, 4.5, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|    31|[(1024,[836,837,8...|[3.0, 4.0, 4.0, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|   516|[(1024,[15,143,14...|[5.0, 4.0, 4.0, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|    85|[(1024,[219,668],...|[5.0, 5.0, 5.0, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|   137|[(1024,[89,143,14...|[4.0, 5.0, 3.5, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|   251|[(1024,[263,307,4...|[5.0, 5.0, 5.0, 5...|[[0.0, 0.0, 0.0, ...|\n",
      "|   451|[(1024,[156,273,4...|[5.0, 3.0, 4.0, 3...|[[0.0, 0.0, 0.0, ...|\n",
      "|   580|[(1024,[156,273,4...|[3.0, 4.0, 3.5, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|    65|[(1024,[65,219,38...|[4.0, 4.0, 4.5, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "|   458|[(1024,[204,428,5...|[4.0, 3.0, 5.0, 5...|[[0.0, 0.0, 0.0, ...|\n",
      "|    53|[(1024,[867],[0.9...|[5.0, 5.0, 5.0, 5...|[[0.0, 0.0, 0.0, ...|\n",
      "|   255|[(1024,[219,438,6...|[1.0, 2.0, 5.0, 2...|[[0.0, 0.0, 0.0, ...|\n",
      "|   481|[(1024,[605,668,9...|[3.0, 4.0, 2.0, 2...|[[0.0, 0.0, 0.0, ...|\n",
      "|   588|[(1024,[449,759,8...|[3.0, 5.0, 3.0, 4...|[[0.0, 0.0, 0.0, ...|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_coef.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "514ce682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "\n",
    "def lr_apply(x: SparseVector, lr_coef: list) -> float:\n",
    "    \"\"\"\n",
    "        param x: векторр фич для регрессии\n",
    "        param lr_coef: \n",
    "        return предсказанное моделью регрессии значение\n",
    "    \"\"\"\n",
    "    return float(np.array(x).dot(np.array(lr_coef[0])) + lr_coef[1][0])\n",
    "\n",
    "lr_apply_udf = sql_func.udf(lr_apply, returnType=ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "535098d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def get_prediction(data: DataFrame) -> float:\n",
    "    return(\n",
    "        data\n",
    "        .join(model_coef, \"userId\")\n",
    "        .select(\n",
    "            \"userId\",\n",
    "            \"rating\",\n",
    "            \"movieId\",\n",
    "            \"tf_idf\",\n",
    "            lr_apply_udf(\"tf_idf\", \"model_coef\").alias(\"prediction\"))\n",
    "        .cache()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15a992ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction = get_prediction(train_data)\n",
    "(\n",
    "    train_prediction.write.mode(\"overwrite\")\n",
    "    .parquet(os.path.join(DATA_DIR, \"train_prediction.parquet\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91705750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+--------------------+----------+\n",
      "|userId|rating|movieId|              tf_idf|prediction|\n",
      "+------+------+-------+--------------------+----------+\n",
      "|     1|   4.0|      1|(1024,[156,273,42...|      null|\n",
      "|     5|   4.0|      1|(1024,[156,273,42...|      null|\n",
      "|    15|   2.5|      1|(1024,[156,273,42...|      null|\n",
      "|    17|   4.5|      1|(1024,[156,273,42...|      null|\n",
      "|    18|   3.5|      1|(1024,[156,273,42...|      null|\n",
      "|    21|   3.5|      1|(1024,[156,273,42...|      null|\n",
      "|    31|   5.0|      1|(1024,[156,273,42...|      null|\n",
      "|    32|   3.0|      1|(1024,[156,273,42...|      null|\n",
      "|    33|   3.0|      1|(1024,[156,273,42...|      null|\n",
      "|    40|   5.0|      1|(1024,[156,273,42...|      null|\n",
      "|    44|   3.0|      1|(1024,[156,273,42...|      null|\n",
      "|    45|   4.0|      1|(1024,[156,273,42...|      null|\n",
      "|    46|   5.0|      1|(1024,[156,273,42...|      null|\n",
      "|    50|   3.0|      1|(1024,[156,273,42...|      null|\n",
      "|    54|   3.0|      1|(1024,[156,273,42...|      null|\n",
      "|    63|   5.0|      1|(1024,[156,273,42...|      null|\n",
      "|    64|   4.0|      1|(1024,[156,273,42...|      null|\n",
      "|    66|   4.0|      1|(1024,[156,273,42...|      null|\n",
      "|    71|   5.0|      1|(1024,[156,273,42...|      null|\n",
      "|    73|   4.5|      1|(1024,[156,273,42...|      null|\n",
      "+------+------+-------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00df322b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(prediction: DataFrame) -> float:\n",
    "    return np.sqrt(\n",
    "        prediction\n",
    "        .selectExpr(\"\"\"\n",
    "        CASE\n",
    "            WHEN prediction > 5. THEN 5.\n",
    "            WHEN prediction < .5 THEN .5\n",
    "            ELSE prediction\n",
    "        END AS prediction\n",
    "        \"\"\", \"rating\")\n",
    "        .select(\n",
    "            sql_func.pow(sql_func.col(\"rating\") - sql_func.col(\"prediction\"), 2)\n",
    "            .alias(\"squared error\")\n",
    "        )\n",
    "        .agg(sql_func.avg(\"squared_error\"))\n",
    "        .first()[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95221ef9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(`prediction` > 5BD)' due to data type mismatch: differing types in '(`prediction` > 5BD)' (array<float> and decimal(1,0)).; line 3 pos 17;\n'Project [CASE WHEN (prediction#3302 > 5) THEN 5 WHEN (prediction#3302 < 0.5) THEN 0.5 ELSE prediction#3302 END AS prediction#3660, rating#18]\n+- Project [userId#16, rating#18, movieId#17, tf_idf#1440, lr_apply(tf_idf#1440, model_coef#1814) AS prediction#3302]\n   +- Project [userId#16, movieId#17, rating#18, timestamp#19, title#105, tf_idf#1440, x#1807, y#1809, model_coef#1814]\n      +- Join Inner, (userId#16 = userId#3288)\n         :- Sample 0.0, 0.8, false, 42\n         :  +- Sort [movieId#17 ASC NULLS FIRST, userId#16 ASC NULLS FIRST, rating#18 ASC NULLS FIRST, timestamp#19 ASC NULLS FIRST, title#105 ASC NULLS FIRST, tf_idf#1440 ASC NULLS FIRST], false\n         :     +- Project [movieId#17, userId#16, rating#18, timestamp#19, title#105, tf_idf#1440]\n         :        +- Join Inner, (movieId#17 = movieId#104)\n         :           :- Relation[userId#16,movieId#17,rating#18,timestamp#19] csv\n         :           +- Project [movieId#104, title#105, tf_idf#1440]\n         :              +- Project [movieId#104, title#105, content_features#214, term_frequencies#295, UDF(term_frequencies#295) AS tf_idf#1440]\n         :                 +- Project [movieId#104, title#105, content_features#214, UDF(content_features#214) AS term_frequencies#295]\n         :                    +- Project [movieId#104, title#105, <lambda>(genres_list#110, tags_list#124) AS content_features#214]\n         :                       +- Project [movieId#104, tags_list#124, title#105, genres_list#110]\n         :                          +- Join RightOuter, (movieId#61 = movieId#104)\n         :                             :- Aggregate [movieId#61], [movieId#61, collect_list(tag#118, 0, 0) AS tags_list#124]\n         :                             :  +- Project [movieId#61, upper(tag#62) AS tag#118]\n         :                             :     +- Relation[userId#60,movieId#61,tag#62,timestamp#63] csv\n         :                             +- Project [movieId#104, title#105, genres_list#110]\n         :                                +- Project [movieId#104, title#105, genres#106, split(genres#106, \\|, -1) AS genres_list#110]\n         :                                   +- Relation[movieId#104,title#105,genres#106] csv\n         +- Project [userId#3288, x#1807, y#1809, sklearn_lr(x#1807, y#1809) AS model_coef#1814]\n            +- Aggregate [userId#3288], [userId#3288, collect_list(tf_idf#1440, 0, 0) AS x#1807, collect_list(rating#3290, 0, 0) AS y#1809]\n               +- Sample 0.0, 0.8, false, 42\n                  +- Sort [movieId#3289 ASC NULLS FIRST, userId#3288 ASC NULLS FIRST, rating#3290 ASC NULLS FIRST, timestamp#3291 ASC NULLS FIRST, title#105 ASC NULLS FIRST, tf_idf#1440 ASC NULLS FIRST], false\n                     +- Project [movieId#3289, userId#3288, rating#3290, timestamp#3291, title#105, tf_idf#1440]\n                        +- Join Inner, (movieId#3289 = movieId#104)\n                           :- Relation[userId#3288,movieId#3289,rating#3290,timestamp#3291] csv\n                           +- Project [movieId#104, title#105, tf_idf#1440]\n                              +- Project [movieId#104, title#105, content_features#214, term_frequencies#295, UDF(term_frequencies#295) AS tf_idf#1440]\n                                 +- Project [movieId#104, title#105, content_features#214, UDF(content_features#214) AS term_frequencies#295]\n                                    +- Project [movieId#104, title#105, <lambda>(genres_list#110, tags_list#124) AS content_features#214]\n                                       +- Project [movieId#104, tags_list#124, title#105, genres_list#110]\n                                          +- Join RightOuter, (movieId#61 = movieId#104)\n                                             :- Aggregate [movieId#61], [movieId#61, collect_list(tag#118, 0, 0) AS tags_list#124]\n                                             :  +- Project [movieId#61, upper(tag#62) AS tag#118]\n                                             :     +- Relation[userId#60,movieId#61,tag#62,timestamp#63] csv\n                                             +- Project [movieId#104, title#105, genres_list#110]\n                                                +- Project [movieId#104, title#105, genres#106, split(genres#106, \\|, -1) AS genres_list#110]\n                                                   +- Relation[movieId#104,title#105,genres#106] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0328b6bba772>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluate_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-d449bfdf3ed7>\u001b[0m in \u001b[0;36mevaluate_prediction\u001b[1;34m(prediction)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     return np.sqrt(\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         .selectExpr(\"\"\"\n\u001b[0;32m      5\u001b[0m         \u001b[0mCASE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselectExpr\u001b[1;34m(self, *expr)\u001b[0m\n\u001b[0;32m   1433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1434\u001b[0m             \u001b[0mexpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1435\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1436\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.0.3-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '(`prediction` > 5BD)' due to data type mismatch: differing types in '(`prediction` > 5BD)' (array<float> and decimal(1,0)).; line 3 pos 17;\n'Project [CASE WHEN (prediction#3302 > 5) THEN 5 WHEN (prediction#3302 < 0.5) THEN 0.5 ELSE prediction#3302 END AS prediction#3660, rating#18]\n+- Project [userId#16, rating#18, movieId#17, tf_idf#1440, lr_apply(tf_idf#1440, model_coef#1814) AS prediction#3302]\n   +- Project [userId#16, movieId#17, rating#18, timestamp#19, title#105, tf_idf#1440, x#1807, y#1809, model_coef#1814]\n      +- Join Inner, (userId#16 = userId#3288)\n         :- Sample 0.0, 0.8, false, 42\n         :  +- Sort [movieId#17 ASC NULLS FIRST, userId#16 ASC NULLS FIRST, rating#18 ASC NULLS FIRST, timestamp#19 ASC NULLS FIRST, title#105 ASC NULLS FIRST, tf_idf#1440 ASC NULLS FIRST], false\n         :     +- Project [movieId#17, userId#16, rating#18, timestamp#19, title#105, tf_idf#1440]\n         :        +- Join Inner, (movieId#17 = movieId#104)\n         :           :- Relation[userId#16,movieId#17,rating#18,timestamp#19] csv\n         :           +- Project [movieId#104, title#105, tf_idf#1440]\n         :              +- Project [movieId#104, title#105, content_features#214, term_frequencies#295, UDF(term_frequencies#295) AS tf_idf#1440]\n         :                 +- Project [movieId#104, title#105, content_features#214, UDF(content_features#214) AS term_frequencies#295]\n         :                    +- Project [movieId#104, title#105, <lambda>(genres_list#110, tags_list#124) AS content_features#214]\n         :                       +- Project [movieId#104, tags_list#124, title#105, genres_list#110]\n         :                          +- Join RightOuter, (movieId#61 = movieId#104)\n         :                             :- Aggregate [movieId#61], [movieId#61, collect_list(tag#118, 0, 0) AS tags_list#124]\n         :                             :  +- Project [movieId#61, upper(tag#62) AS tag#118]\n         :                             :     +- Relation[userId#60,movieId#61,tag#62,timestamp#63] csv\n         :                             +- Project [movieId#104, title#105, genres_list#110]\n         :                                +- Project [movieId#104, title#105, genres#106, split(genres#106, \\|, -1) AS genres_list#110]\n         :                                   +- Relation[movieId#104,title#105,genres#106] csv\n         +- Project [userId#3288, x#1807, y#1809, sklearn_lr(x#1807, y#1809) AS model_coef#1814]\n            +- Aggregate [userId#3288], [userId#3288, collect_list(tf_idf#1440, 0, 0) AS x#1807, collect_list(rating#3290, 0, 0) AS y#1809]\n               +- Sample 0.0, 0.8, false, 42\n                  +- Sort [movieId#3289 ASC NULLS FIRST, userId#3288 ASC NULLS FIRST, rating#3290 ASC NULLS FIRST, timestamp#3291 ASC NULLS FIRST, title#105 ASC NULLS FIRST, tf_idf#1440 ASC NULLS FIRST], false\n                     +- Project [movieId#3289, userId#3288, rating#3290, timestamp#3291, title#105, tf_idf#1440]\n                        +- Join Inner, (movieId#3289 = movieId#104)\n                           :- Relation[userId#3288,movieId#3289,rating#3290,timestamp#3291] csv\n                           +- Project [movieId#104, title#105, tf_idf#1440]\n                              +- Project [movieId#104, title#105, content_features#214, term_frequencies#295, UDF(term_frequencies#295) AS tf_idf#1440]\n                                 +- Project [movieId#104, title#105, content_features#214, UDF(content_features#214) AS term_frequencies#295]\n                                    +- Project [movieId#104, title#105, <lambda>(genres_list#110, tags_list#124) AS content_features#214]\n                                       +- Project [movieId#104, tags_list#124, title#105, genres_list#110]\n                                          +- Join RightOuter, (movieId#61 = movieId#104)\n                                             :- Aggregate [movieId#61], [movieId#61, collect_list(tag#118, 0, 0) AS tags_list#124]\n                                             :  +- Project [movieId#61, upper(tag#62) AS tag#118]\n                                             :     +- Relation[userId#60,movieId#61,tag#62,timestamp#63] csv\n                                             +- Project [movieId#104, title#105, genres_list#110]\n                                                +- Project [movieId#104, title#105, genres#106, split(genres#106, \\|, -1) AS genres_list#110]\n                                                   +- Relation[movieId#104,title#105,genres#106] csv\n"
     ]
    }
   ],
   "source": [
    "evaluate_prediction(train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = get_prediction(test_data)\n",
    "(\n",
    "    test_prediction.write.mode(\"overwrite\")\n",
    "    .parquet(os.path.join(DATA_DIR, \"test_prediction.parquet\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prediction(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763109ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.where(\"title LIKE 'Top gun%'\").select(\"title\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72b728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
