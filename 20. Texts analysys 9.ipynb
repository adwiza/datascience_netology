{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03bdc8e8",
   "metadata": {},
   "source": [
    "# Классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead475e5",
   "metadata": {},
   "source": [
    "## Сеть прямого распространения  для классификации текстов\n",
    "\n",
    "\n",
    "![title](img/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac0008",
   "metadata": {},
   "source": [
    "* $x$ - входное векторное представление текста\n",
    "* $h$ – скрытые слои с нелинейными функциями активации\n",
    "* $y$ – выходы, как правило, один $y$ соответствует одной метке класса \n",
    "\n",
    "$NN_{MLP2}(x) = y$\n",
    "\n",
    "$h_1 = g^1(xW^1 + b^1)$\n",
    "\n",
    "$h_2 = g^2(h^1 W^2 + b^2)$\n",
    "\n",
    "$y = h^2 W^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ea97b",
   "metadata": {},
   "source": [
    "### Нелинейные функции активации\n",
    "\n",
    "![title](img/activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a5a11",
   "metadata": {},
   "source": [
    "### Обучение сети \n",
    "### Алгоритм обратного распространения ошибки \n",
    "\n",
    "Ошибка: cross entropy: $\\text{loss}(y_{true}, \\hat{y}_{pred}) = \\sum y_{true} \\log(\\hat{y}_{pred})$ \n",
    "\n",
    "1. Прямой проход:\n",
    "    * вычислить $\\hat{y}_{pred}$ с текущими весами на скрытых слоях\n",
    "    * оценить $\\text{loss}(y_{true}, \\hat{y}_{pred})$\n",
    "\n",
    "2. Обратный проход:\n",
    "    * оценить  $\\Delta W_h$ на каждом скрытом слое\n",
    "    \n",
    "    $\\Delta W_h = \\frac {\\partial  \\text{loss}}{\\partial W_{H}} = \\frac{\\partial \\text{loss}}{\\partial \\hat{y}_{pred}}  \\frac{\\partial \\hat{y}_{pred}}{\\partial W_{H}} $\n",
    "    \n",
    "    * обновление весов: $ \\Delta W_H =-\\eta {\\frac {\\partial \\text{loss} } {\\partial W_H} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b4a337",
   "metadata": {},
   "source": [
    "### dropout-регуляризация\n",
    "\n",
    "$NN_{MLP2}(x) = y$\n",
    "\n",
    "$h_1 = g^1(xW^1 + b^1)$\n",
    "\n",
    "$m^1 $~$ Bernouli(r^1)$\n",
    "\n",
    "$\\hat{h^1} = m^1 \\odot h^1$\n",
    "\n",
    "$h_2 = g^2(\\hat{h^1} W^2 + b^2)$\n",
    "\n",
    "$m^2 $~$  Bernouli(r^2)$\n",
    "\n",
    "$\\hat{h^2} = m^2 \\odot h^2$\n",
    "\n",
    "$y =\\hat{h^2} W^3$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8912b0",
   "metadata": {},
   "source": [
    "### Векторное представление текста \n",
    "\n",
    "\n",
    "1. Мешок слов [Bag of Words, BoW]\n",
    "    * $|\\text{word} \\in V| = N$ – словарь\n",
    "    * $x \\in D$ – документ, $|x| = k$ \n",
    "    * $\\vec{x}$ – $N$-мерный вектор, $\\vec{x}_i = f(\\text{word}_i, x_i)$, в котором $k$  ненулевых компонент\n",
    "        \\end{itemize}\n",
    "\n",
    "2. Распределенное представление слов [Continuous Bag of Words, CBoW])\n",
    "    * one-hot кодировка: каждое слово $\\text{word}$ – $N$-мерный вектор, $\\overrightarrow{\\text{word}}_i = 1$, иначе – 0\n",
    "    * плотные вектора – эмбеддинги: каждое слово $\\text{word}$ – $d$-мерный вектор, $\\overrightarrow{\\text{word}}_i \\in \\mathbb{R}$\n",
    "\t\n",
    "    Матрица эмбеддингов: $E \\in \\mathbb{R}^{|V| \\times d}$\n",
    "\t\n",
    "    * $\\text{CBOW}(x) = \\frac{1}{k} \\sum_i^k E_i $\n",
    "    * $\\text{x} = [\\overrightarrow{\\text{word}}_1  ,\\ldots, \\overrightarrow{\\text{word}}_k ]$\n",
    "\n",
    "\n",
    "#### Padding\n",
    "Входные тексты имеют переменную длинну, что неудобно, поэтому предположим, что они все состоят из одинакового количества слов, только часть из этих слов – баластные символы pad.\n",
    "\n",
    "\n",
    "#### Неизвестные слова (OOV)\n",
    "Если в тестовом множестве встретилось неизвестное слово, то можно \n",
    "* заменить его на pad;\n",
    "* заменить его на unk.  Однако в обучающем множестве unk никогда не встречается, поэтому его нужно добавить в обучающее множество искусственным образом. \n",
    "\n",
    "\n",
    "#### Word dropout - регуляризация \n",
    "Заменяем каждое слово на unk с вероятностью $\\frac{\\alpha}{|V| + \\alpha}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8344dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "from keras.layers import Embedding, Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "random.seed(1228)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8179500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "regex = re.compile(\"[А-Яа-я:=!\\)\\()A-z\\_\\%/|]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \"\".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \"\n",
    "\n",
    "\n",
    "df_neg = pd.read_csv(\"datasets/nlp/negative.csv\", sep=';', header = None, usecols = [3])\n",
    "df_pos = pd.read_csv(\"datasets/nlp/positive.csv\", sep=';', header = None, usecols = [3])\n",
    "df_neg['sent'] = 'neg'\n",
    "df_pos['sent'] = 'pos'\n",
    "df = pd.concat([df_neg, df_pos])\n",
    "df = df[15000:20000]\n",
    "df.columns = ['text', 'sent']\n",
    "df.text = df.text.apply(words_only)\n",
    "df.text = df.text.apply(lemmatize)\n",
    "\n",
    "\n",
    "X = df.text.tolist()\n",
    "y = df.sent.tolist()\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_text_train, X_text_test, y_train, y_test = train_test_split(X,y, test_size=0.33)\n",
    "print (\"total train examples %s\" % len(y_train))\n",
    "print (\"total test examples %s\" % len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2debd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_LENGTH = 10\n",
    "VOCABULARY_SIZE = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "DIMS = 250\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "batch_size = 32\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c445a",
   "metadata": {},
   "source": [
    "## Сеть прямого распостранения\n",
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0116dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(X_text_train)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3eb115",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_text_train)\n",
    "X_train = tokenizer.sequences_to_matrix(sequences, mode='count')\n",
    "sequences = tokenizer.texts_to_sequences(X_text_test)\n",
    "X_test = tokenizer.sequences_to_matrix(sequences, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450904a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First seq:', sequences[0])\n",
    "print('First doc:', X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(['pos', 'neg'])\n",
    "y_train_cat = np_utils.to_categorical(le.transform(y_train), 2)\n",
    "y_test_cat = np_utils.to_categorical(le.transform(y_test), 2)\n",
    "\n",
    "\n",
    "print(y_train_cat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(MAX_FEATURES, ), activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train_cat, epochs=nb_epoch, batch_size=batch_size, validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b56f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_test)\n",
    "pred = le.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe32e77",
   "metadata": {},
   "source": [
    "### CBoW - слечайно инициализированные эмбеддиги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5482af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_text_train)\n",
    "X_train = pad_sequences(sequences, mexlen=TEXT_LENGTH)\n",
    "sequences = tokenizer.texts_to_sequences(X_text_test)\n",
    "X_test = pad_sequences(sequnces, maxlen=TEXT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f4e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb27218",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequental()\n",
    "model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, input_length=TEXT_LENGTH, trainable=True ))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss=('categorical_crossentropy', optimizer='adam', metrics=['accuracy']))\n",
    "model.fit(X_train, y_train_cat, epochs=nb_epochs, batch_size=batch_size, validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1462177",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_test)\n",
    "pred = le.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882beb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "emb_path = 'datasets/nlp/wiki.ru.vec'\n",
    "\n",
    "words = []\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(emb_path)\n",
    "for line in f:\n",
    "    values - line.split()\n",
    "    if len(values) == 301:\n",
    "        word = values[0]\n",
    "        words.append(words)\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=TEXT_LENGTH,\n",
    "                            trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train_cat, epochs=nb_epoch, batch_size=batch_size,  validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab00dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
