{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8695df1",
   "metadata": {},
   "source": [
    "# Токенизация и подсчет количества слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9f6d6",
   "metadata": {},
   "source": [
    "### Сколько слов в этом предложении?\n",
    "\n",
    "* На дворе трава, на траве дрова, не руби дрова на траве двора.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b67ae9b",
   "metadata": {},
   "source": [
    "<b> 12 токенов </b> На, дворе, трава, на, траве, дрова, не, руби, дрова, на, траве, двора<br>\n",
    "<b> 8 - 9 типов </b> Н/на, дворе, трава, траве, дрова, не, руби, дрова<br>\n",
    "<b> 6 лексем </b>на, не, двор, трава, дрова, рубить"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5c4f6",
   "metadata": {},
   "source": [
    "#### Токен и тип\n",
    "<b>Тип</b> - уникальное слово из текста<br>\n",
    "<b>Токен</b> - тип и его позиция в тексте"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab119c",
   "metadata": {},
   "source": [
    "#### Обозначения\n",
    "N = число токенов\n",
    "V - словарь (все типы)\n",
    "|V| = число типов в словаре\n",
    "\n",
    "<b>Как связаны N и |V|?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603c50dc",
   "metadata": {},
   "source": [
    "<b>Закон Ципфа</b>\n",
    "В любом достаточно большом тексте ранг типа обратно пропорционален его частотности $f = \\frac {a}{r}$<br>\n",
    "$f$ - частота типа, $r$ - ранг типа, $a$ - параметр для славянских языков - около 0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a366292",
   "metadata": {},
   "source": [
    "<b>Закон Хипса</b>\n",
    "С увеличением длины текста {количества токнов}, количество типов увеличивается в соответствии с законом $|V| =  {K}*{N^b}$<br>\n",
    "$N$ - число токенов, $|V|$ - количество типов в словаре, $K,b$ - параметры, обычно K $\\in$ [10, 100], b $\\in$ [0.4, 0.6] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3725a25",
   "metadata": {},
   "source": [
    "## Анализ сообщений VK.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fefdf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/nlp/vk_texts_with_sources.csv', usecols=['text', 'source'])\n",
    "\n",
    "df.text.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8599c",
   "metadata": {},
   "source": [
    "## Предварительный анализ коллекции\n",
    "\n",
    "### Средняя длина текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.astype(str)\n",
    "len_data = df.text.apply(len)\n",
    "len_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99869097",
   "metadata": {},
   "source": [
    "### Количество текстов разных пабликов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "counts = df.source.value_counts()\n",
    "values = counts.tolist()\n",
    "labels = counts.index.tolist()\n",
    "\n",
    "y_pos = np.arange(len(labels))\n",
    "\n",
    "plt.bar(y_pos, values, align='center', alpha=.5)\n",
    "plt.xticks(y_pos, range(len(labels)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27feac1",
   "metadata": {},
   "source": [
    "### Длины текстов в символах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13600be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "length = len_data[len_data < 10000].tolist()\n",
    "\n",
    "n, bins, patches = ax.hist(length)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89d4bc",
   "metadata": {},
   "source": [
    "### Токенизация\n",
    "\n",
    "Используем регулярные выражения, чтобы разбить текст на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a44280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile('[А-Яа-я]+')\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return ' '.join(regex.findall(text))\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "df.text = df.text.str.lower()\n",
    "df.text = df.text.apply(words_only)\n",
    "\n",
    "df.text.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76784830",
   "metadata": {},
   "source": [
    "### Самые частые слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d99786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "n_types = []\n",
    "n_tokens = []\n",
    "fd = FreqDist()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tokens = row['text'].split()\n",
    "    fd.update(tokens)\n",
    "    n_types.append(len(fd))\n",
    "    n_tokens.append(sum(fd.values()))\n",
    "\n",
    "for i in fd.most_common(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac01779",
   "metadata": {},
   "source": [
    "### Закон Ципфа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89093fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = list(fd.values())\n",
    "freqs = sorted(freqs, reverse=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(freqs[:300], range(300))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb416878",
   "metadata": {},
   "source": [
    "### Сегментация предложений\n",
    "\"!\",\"?\" как правило однозначны, проблемы возникают с \".\"\n",
    "Бинарный классификатор для сегментации преллодений для каждой точки \".\" лпределить, является ли она концом \n",
    "предложения или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Первое предложение. Второе предложение! И, наконец, третье? В четвертом предложении г. Москва встречается т. к. это город.\"\n",
    "sents = sent_tokenize(text)\n",
    "\n",
    "print(len(sents))\n",
    "print(*sents, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde3d50",
   "metadata": {},
   "source": [
    "### Задание 2\n",
    "\n",
    "Посчитайте количество предложений, токенов и типов из файла task2.txt, Сохраните список токенов в массив tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import FreqDist\n",
    "\n",
    "fd = FreqDist()\n",
    "regex = re.compile('[А-Яа-я]+')\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return ' '.join(regex.findall(text))\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "text = ' '.join([line.strip() for line in open('datasets/nlp/task2.txt', encoding='utf8')])\n",
    "tokens = words_only(text)\n",
    "sents = sent_tokenize(text)\n",
    "print(len(sents))\n",
    "d1 = nltk.FreqDist(tokens)\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62520a",
   "metadata": {},
   "source": [
    "### Частотный анализ текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f824092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "d1 = nltk.FreqDist(tokens) # частотный словарь для текстов\n",
    "d1.most_common(10) # токен и  его количество появлений в тексте\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a20578",
   "metadata": {},
   "source": [
    "## Морфологический анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cdcc0b",
   "metadata": {},
   "source": [
    "### Задачи морфологи ческого аннализа\n",
    "\n",
    "* Разбор слов - определение нормальной формы (леммы), основы (стема) и грамматических характеристик слова\n",
    "* Синтез слова - генерация слова по заданным грамматическим характеристикам\n",
    "\n",
    "### Морфологический процессор - инструмент морфологического анализа\n",
    "* Морфологический словарь\n",
    "* Морфологический анализатор\n",
    "\n",
    "### Лемматизация\n",
    "У каждого слова есть лемма (нормальная форма)\n",
    "* кошке, кошку, кошкам, кошкой $\\implies$ кошка\n",
    "* бежал, бежит, бегу $\\implies$ бежать\n",
    "* белому, белым, белыми $\\implies$ белый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8223a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'Действительно, на его лице не отражалось никаких чувств – ни проблеска сочувствия не было на нем, \\\n",
    "а ведь боль просто невыносима'\n",
    "sent2 = 'У страха глаза велики .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "m = MorphAnalyzer()\n",
    "lemma1 = [m.parse(word)[0].normal_form for word in sent1.split()]\n",
    "print(' '.join(lemma1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10886a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "m  = Mystem()\n",
    "lemma2 = m.lemmatize(sent1)\n",
    "print(''.join(lemma2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb6bd7",
   "metadata": {},
   "source": [
    "### Стемминг\n",
    "Слова состоят из морфем:  $word = stem + affixes$. Стемминг позволяет отбросить аффиксы. Чаще всего используется алгоритм Портера.\n",
    "* 1-ый вид ошибки: белый, белка, белье $\\implies$  бел\n",
    "\n",
    "* 2-ой вид ошибки: трудность, трудный $\\implies$  трудност, труд \n",
    "\n",
    "* 3-ий вид ошибки: быстрый, быстрее $\\implies$  быст, побыстрее $\\implies$  побыст\n",
    "\n",
    "Алгоритм Портера состоит из 5 циклов команд, на каждом цикле – операция удаления / замены суффикса. Возможны вероятностные расширения алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94396bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "stemmer = RussianStemmer()\n",
    "words = ['распределение', 'приставить', 'сделала', 'словообразование']\n",
    "for w in words:\n",
    "    stem = stemmer.stem(w)\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9036e167",
   "metadata": {},
   "source": [
    "#### Разбор слова "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79266e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'ГАИ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f407ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "m.parse(word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce60ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "m.analyze(word1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407d757",
   "metadata": {},
   "source": [
    "#### Задание 4\n",
    "Найти в списке персонажей \"Война и мир\" (task3.txt) все уникальные женские имена\n",
    "\n",
    "### Первичная обработка текста\n",
    "\n",
    "#### Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = Mystem()\n",
    "m = MorphAnalyzer()\n",
    "text = ' '.join([line.strip() for line in open('datasets/nlp/task3.txt', encoding='utf8')])\n",
    "tokens = words_only(text)\n",
    "\n",
    "#  [{'analysis': [{'lex': 'вера', 'wt': 0.6768655918, 'gr': 'S,имя,жен,од=им,ед'}], 'text': 'Вера'}, {'text': '\\n'}]\n",
    "names = set()\n",
    "prog = re.compile('[А-Я]{1}[а-я]+') # Слово с заглавной буквы\n",
    "tokens = prog.findall(text)\n",
    "lemmas = [m.parse(word)[0].normal_form for word in tokens]\n",
    "\n",
    "for word in lemmas:\n",
    "    if {'Name','femn'} in m.parse(word)[0].tag:\n",
    "        names.add(word.capitalize())\n",
    "        \n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('Russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50538d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystopwords = stopwords.words('russian') + ['это', 'наш' , 'тыс', 'млн', 'млрд', 'также',  'т', 'д']\n",
    "def remove_stopwords(text, mystopwords=mystopwords):\n",
    "    try:\n",
    "        return ' '.join([token for token in text.split() if token in mystopwords])\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eecd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return ''.join(m.lemmatize(text)).strip()\n",
    "    except:\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be27aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystoplemmas = ['который','прошлый','сей', 'свой', 'наш', 'мочь']\n",
    "def remove_stoplemmas(text, mystoplemmas=mystoplemmas):\n",
    "    try:\n",
    "        return \" \".join([token for token in text.split() if not token in mystoplemmas])\n",
    "    except:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96fd284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "df.text = tqdm(df.text.apply(remove_stopwords))\n",
    "df.text = tqdm(df.text.apply(lemmatize))\n",
    "df.text = tqdm(df.text.apply(remove_stoplemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e84337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9dbba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmata = []\n",
    "for index, row in df.iterrows():\n",
    "    lemmata += row['text'].split()\n",
    "\n",
    "fd = FreqDist(lemmata)\n",
    "for i in fd.most_common(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf27719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
