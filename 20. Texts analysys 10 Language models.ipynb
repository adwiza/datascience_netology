{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187bccb9",
   "metadata": {},
   "source": [
    "# Языковые модели\n",
    "\n",
    "Какое слово в последовательности вероятнее: \n",
    "\n",
    "Поезд прибыл на\n",
    "* вокзал\n",
    "* север\n",
    "\n",
    "Какая последовательность вероятнее:\n",
    "* Вокзал прибыл поезд на\n",
    "* Поезд прибыл на вокзал\n",
    "\n",
    "Языковая модель [language model, LM]  позволяет оценить вероятность следующего слова в последовательности  $P(w_n | w_1, \\ldots, w_{n-1})$ и оценить вероятность всей последовательности слов $P(w_1, \\ldots, w_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5b89a",
   "metadata": {},
   "source": [
    "### Приложения:\n",
    "* Задачи, в которых нужно обработать сложный и зашумленный вход: распознавание речи, распознавание сканированных и рукописных текстов;\n",
    "* Исправление опечаток\n",
    "* Машинный перевод\n",
    "* Подсказка при наборе "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c07c4",
   "metadata": {},
   "source": [
    "### Виды моделей:\n",
    "* Счетные модели\n",
    "    * цепи Маркова\n",
    "* Нейронные модели, обычно реккурентные нейронные сети с LSTM/GRU\n",
    "* seq2seq архитектуры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f4630",
   "metadata": {},
   "source": [
    "## Модель $n$-грам\n",
    "\n",
    "Пусть $w_{1:n}=w_1,\\ldots,w_m$ – последовательность слов.\n",
    "\n",
    "Цепное правило:\n",
    "$ P(w_{1:m}) = P(w_1) P(w_2 | w_1) P(w_3 | w_{1:2}) \\ldots P(w_m | w_{1:m-1}) = \\prod_{k=1}^{m} P(w_k | w_{1:k-1}) $\n",
    "\n",
    "Но оценить $P(w_k | w_{1:k-1})$ не легче! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4adbe3",
   "metadata": {},
   "source": [
    "Переходим к $n$-грамам: $P(w_{i+1} | w_{1:i}) \\approx P(w_{i+1} | w_{i-n:i})  $ , то есть, учитываем $n-1$ предыдущее слово. \n",
    "\n",
    "Модель\n",
    "* униграм: $P(w_k)$\n",
    "* биграм: $P(w_k | w_{k-1})$\n",
    "* триграм: $P(w_k | w_{k-1} w_{k-2})$\n",
    "\n",
    "\n",
    "Т.е. используем Марковские допущения о длине запоминаемой цепочки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b5539",
   "metadata": {},
   "source": [
    "* Вероятность следующего слова в последовательности: $ P(w_{i+1} | w_{1:i}) \\approx P(w_{i-n:i}) $\n",
    "* Вероятность всей последовательности слов: $P(w_{1:n}) = \\prod_{k=1}^l P(w_k | w_{k-n+1: k-1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd2256",
   "metadata": {},
   "source": [
    "### Качество модели  $n$-грам\n",
    "\n",
    "Перплексия: насколько хорошо модель предсказывает выборку. Чем ниже значение перплексии, тем лучше.\n",
    "\n",
    "$PP(\\texttt{LM}) = 2 ^ {-\\frac{1}{m} \\log_2 \\texttt{LM} (w_i | w_{1:i-1})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15aa7b5",
   "metadata": {},
   "source": [
    "## Счетные языковые модели\n",
    "\n",
    "### ММП оценки вероятностей\n",
    "$ P_{MLE}(w_k | w_{k-n+1:k-1}) = \\frac{\\texttt{count}(w_{k-n+1:k-1} w_k )}{\\texttt{count}(w_{k-n+1:k-1} )} $\n",
    "\n",
    "В модели биграм:\n",
    "\n",
    "$ P_{MLE}(w_k | w_{k-1}) = \\frac{\\texttt{count}(w_{k-1} w_k )}{\\texttt{count}(w_{k-1} )} $\n",
    "\n",
    "Возникает проблема нулевых вероятностей!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a11868",
   "metadata": {},
   "source": [
    "### Аддитивное сглаживание Лапласа\n",
    "\n",
    "$ P(w_k | w_{k-1}) = \\frac{\\texttt{count}(w_{k-1} w_k ) + \\alpha}{\\texttt{count}(w_{k-1} ) + \\alpha |V|} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdb508",
   "metadata": {},
   "source": [
    "## Модели биграм в NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e2f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, TimeDistributed, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a901567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aachenosaurus', 'aardonyx', 'abdallahsaurus', 'abelisaurus', 'abrictosaurus', 'abrosaurus', 'abydosaurus', 'acanthopholis', 'achelousaurus', 'acheroraptor']\n"
     ]
    }
   ],
   "source": [
    "names = [name.strip().lower() for name in \n",
    "         open('C:/Users/adwiz/Documents/Courses/datascience_netology/datasets/nlp/dinos.txt').readlines()]\n",
    "print(names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b60a66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'c', 'h', 'e', 'n', 'o', 's', 'u', 'r', 'd', 'y', 'x', 'b', 'l', 'i', 't', 'p', 'v', 'm', 'g', 'f', 'j', 'k', 'w', 'z', 'q']\n"
     ]
    }
   ],
   "source": [
    "chars = [char for name in names for char in name]\n",
    "freq = nltk.FreqDist(chars)\n",
    "print(list(freq.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1020ed76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'u': 792, 'n': 354, 't': 213, 's': 187, 'l': 146, 'r': 131, 'c': 109, 'p': 96, 'm': 74, 'e': 48, ...})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfreq = nltk.ConditionalFreqDist(nltk.bigrams(chars))\n",
    "cfreq['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efca740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(a a) = 0.0105\n",
      "p(a b) = 0.0129\n",
      "p(a u) = 0.3185\n"
     ]
    }
   ],
   "source": [
    "cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)\n",
    "print('p(a a) = %1.4f' %cprob['a'].prob('a'))\n",
    "print('p(a b) = %1.4f' %cprob['a'].prob('b'))\n",
    "print('p(a u) = %1.4f' %cprob['a'].prob('u'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe71f11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.041317008359863"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "log(cprob['a'].prob('a')) + log(cprob['a'].prob('b')) + log(cprob['a'].prob('c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12f2a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(a) = 0.1354\n"
     ]
    }
   ],
   "source": [
    "l = sum([freq[char] for char in freq])\n",
    "def unigram_prob(char):\n",
    "    return freq[char] / l\n",
    "print(f'p(a) = {unigram_prob(\"a\"):1.4f}')\n",
    "# print('p(a) = %1.4f' %unigram_prob('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7622a4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'a'),\n",
       " ('a', 'c'),\n",
       " ('c', 'h'),\n",
       " ('h', 'e'),\n",
       " ('e', 'n'),\n",
       " ('n', 'o'),\n",
       " ('o', 's'),\n",
       " ('s', 'a'),\n",
       " ('a', 'u'),\n",
       " ('u', 'r'),\n",
       " ('r', 'u'),\n",
       " ('u', 's')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bi for bi in nltk.bigrams('aachenosaurus')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56362bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cprob[\"a\"].generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce6b1bc",
   "metadata": {},
   "source": [
    "#### Задание 1\n",
    "\n",
    "1. Напишите функцию для оценки вероятности имени динозавра. \n",
    "2. Найдите наиболее вероятное имя динозавра из данного списка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81c379ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_prob(name):\n",
    "    p = unigram_prob(name[0])\n",
    "    for i in range(len(name) - 1):\n",
    "        p *= cprob[name[i]].prob(name[i+1])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99bc624d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0222358416238476e-10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_prob(names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab6661ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aachenosaurus'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "063ac870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tesangowur'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_name(cprob, first_char, num_chars):\n",
    "    name = ''\n",
    "    name += first_char\n",
    "    for i in range(num_chars):\n",
    "        char = cprob[first_char].generate()\n",
    "        name += char\n",
    "        first_char = char\n",
    "    return name\n",
    "\n",
    "generate_name(cprob, 't', 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353182d",
   "metadata": {},
   "source": [
    "## Нейронные языковые модели\n",
    "\n",
    "* Вход: $n$-грамы $w_{1:k}$\n",
    "* $v(w_i)$ – эмбеддинг слова $w_i$, $v(w_i) \\in \\mathbb{R}^{d_{emb}}$, $d_{emb}$ – размерность эмбеддинга, $v(w) = E_{[w]}$\n",
    "* $x = [v(w_1), v(w_2), \\ldots , v(w_k)]$\n",
    "\n",
    "$\\widehat{y} = P(w_i | w_{1:k} ) = \\texttt{LM}(w_{1:k}) = \\texttt{softmax}(hW^2 +b^2)$\n",
    "\n",
    "$h = g(xW^1+b^1)$\n",
    "\n",
    "$w_i \\in V$, $E \\in \\mathbb{R}^{|V|\\times d_{emb}}, W^1 \\in \\mathbb{R}^{k \\cdot d_{emb} \\times d_{hid}}, b^1 \\in \\mathbb{R} ^ {d_{hid}}, W^2 \\in \\mathbb{R}^{d_{hid} \\times |V|}, b^2 \\in \\mathbb{R} ^ {|V|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe8841",
   "metadata": {},
   "source": [
    "### Семплирование в нейронных языковых моделях \n",
    "### (Генерация текстов с помощью нейронных языковых моделей)\n",
    "\n",
    "1. Задать начальную последовательность символов длины $k$ (/слов)\n",
    "2. Предсказать распределение вероятностей слов с условием на $k$ предыдущих слов\n",
    "3. 1. Выбрать слово с наибольшей вероятностью\n",
    "3. 2. Выбрать слово по предсказаному распределению\n",
    "4. Сдвинуть окно на одно слово и повторить \n",
    "\n",
    "#### Линейный поиск  (beam search)\n",
    "Всегда помним $h$ наиболее вероятных гипотез:\n",
    "1. Для генерации первого слова в последоватительности генерируем $h$ кандидатов, а не 1\n",
    "2. Генерируем $h \\times h$ кандидатов для второго слова и храним только $h$ наиболее вероятных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "827d001b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 26\n"
     ]
    }
   ],
   "source": [
    "alphabet = list(set(chars))\n",
    "print('total chars:', len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b7e50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb ngrams: 10701\n",
      "a a c h e n\n",
      "a c h e n o\n"
     ]
    }
   ],
   "source": [
    "maxlen = 5\n",
    "step = 1\n",
    "ngrams = []\n",
    "next_chars = []\n",
    "for name in names:\n",
    "    for i in range(0, len(name) - maxlen, step):\n",
    "        ngrams.append(' '.join([char for char in name[i: i + maxlen]]))\n",
    "        next_chars.append(name[i + maxlen])\n",
    "print('nb ngrams:', len(ngrams))\n",
    "print(ngrams[0], next_chars[0])\n",
    "print(ngrams[1], next_chars[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d54fb0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, 12, 11,  7])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(alphabet))\n",
    "tokenizer.fit_on_texts(ngrams)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(ngrams)\n",
    "X_train = pad_sequences(sequences, maxlen=maxlen)\n",
    "sequences = tokenizer.texts_to_sequences(next_chars)\n",
    "y_train = tokenizer.sequences_to_matrix(sequences)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87e460e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b2d3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_index = tokenizer.word_index\n",
    "index_char = {i: c for c, i in char_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0ccc4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(alphabet), 50, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(len(alphabet), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ca7b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1, 100):\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train, y_train)\n",
    "    model.fit(X_train_shuffled, y_train_shuffled, batch_size=len(X_train), epochs=2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f23d9bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) # temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.choice(range(len(alphabet)), p=preds)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a45c844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"aleks\"\n",
      "aleks\n",
      "aleksa\n",
      "aleksau\n",
      "aleksaur\n",
      "aleksauru\n",
      "aleksaurus\n",
      "aleksaurusa\n",
      "aleksaurusau\n",
      "aleksaurusaur\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "generated = ''\n",
    "seed = 'aleks'\n",
    "generated += seed\n",
    "print('----- Generating with seed: \"' + seed + '\"')\n",
    "print(generated)\n",
    "\n",
    "for i in range(8):\n",
    "    sequences = tokenizer.texts_to_sequences([' '.join([char for char in generated[-maxlen:]])])\n",
    "    X_pred = pad_sequences(sequences, maxlen=maxlen)\n",
    "    preds = model.predict(X_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_char = index_char[next_index]\n",
    "    generated += next_char\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40161c98",
   "metadata": {},
   "source": [
    "### Рекуррентные нейронные языковые модели\n",
    "\n",
    "RNN позволяют уйти от Марковских допущений и позволяют учитывать предысторию произвольной длины.\n",
    "\n",
    "$x_{1:n} = x_1, x_2, \\ldots, x_n$, $x_i \\in \\mathbb{R}^{d_{in}}$\n",
    "\n",
    "$y_n = RNN(x_{1:n})$, $y_n \\in \\mathbb{R}^{d_{out}}$\n",
    "\n",
    "Для каждого префикса $x_{i:i}$ $y_i$ – выходной вектор.\n",
    "\n",
    "$y_i = RNN(x_{1:i})$\n",
    "\n",
    "$y_{1:n} = RNN^{*}(x_{1:n})$, $y_i \\in \\mathbb{R}^{d_{out}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afa8ce",
   "metadata": {},
   "source": [
    "#### Управляемые архитектуры\n",
    "\n",
    "RNN трудно обучать: проблема исчезающего градиента. Уйти от нее помогают управляемые нейроны специального вида: LSTM и GRU.\n",
    "\n",
    "$s_i$ – память нейронной сети. Каждое использование $R$ считывает и видоизменяет всю память. \n",
    "\n",
    "Управляемый доступ к памяти: $g \\in {0,1}^n$:\n",
    "\n",
    "$s_{i+1} \\leftarrow g \\odot x + (1-g) \\odot s_{i}$\n",
    "\n",
    "Дифференцируемое управление:\n",
    "\n",
    "$g \\in \\mathbb{R}^n $:\n",
    "\n",
    "$s_{i+1} \\leftarrow \\sigma(g) \\odot x + (1-g) \\odot s_{i}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c16d86",
   "metadata": {},
   "source": [
    "#### Long short-term memory\n",
    "\n",
    "$s_j = R_{LSTM}(s_{j-1}, x_j) = [c_j, h_j]$,  $c_j$ – вектор памяти, $h_j$ – скрытое состояние\n",
    "\n",
    "$c_j = f \\odot c_{j-1} + i \\odot z_j$ – обновление памяти \n",
    "\n",
    "$h_j = o \\odot tanh(c_j)$\n",
    "\n",
    "$i = \\sigma(x_j W^{xi} + h_{j-1} W^{hi}) $ – что нужно выбрать из входа ($i \\odot z$)\n",
    "\n",
    "$f = \\sigma(x_j W^{xf} + h_{j-1} W^{hf}) $ – что нужно забыть ($f \\odot c_{j-1}$)\n",
    "\n",
    "$o = \\sigma(x_j W^{xo} + h_{j-1} W^{ho}) $– что нужно выдать на выход ($o \\odot c_{j}$)\n",
    "\n",
    "$z = tanh(x_j W^{xz} + h_{j-1} W^{hz})$ – обновление \n",
    "\n",
    "$y_j = O_{LSTM}(s_j) = h_j$ – выход \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$x_i \\in \\mathbb{R}^{d_{in}}$, $y_i \\in \\mathbb{R}^{d_{out}}$, $s_i \\in \\mathbb{R}^{2 \\times d_{out}}$\n",
    "\n",
    "$c_j, h_j, i, f, o, z \\in \\mathbb{R}^{ d_{out}}$\n",
    "\n",
    "$W^{x.} \\in \\mathbb{R}^{d_{in} \\times d_{out}}$, $W^{h.} \\in \\mathbb{R}^{d_{out} \\times d_{out}}$\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6da1480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "X_names = ['bos ' + ' '.join(name) for name in names]\n",
    "Y_names = [' '.join(name) + ' eos' for name in names]\n",
    "maxlen = max([len(name) for name in names]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcd9105f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos a a c h e n o s a u r u s',\n",
       " 'bos a a r d o n y x',\n",
       " 'bos a b d a l l a h s a u r u s',\n",
       " 'bos a b e l i s a u r u s',\n",
       " 'bos a b r i c t o s a u r u s',\n",
       " 'bos a b r o s a u r u s',\n",
       " 'bos a b y d o s a u r u s',\n",
       " 'bos a c a n t h o p h o l i s',\n",
       " 'bos a c h e l o u s a u r u s',\n",
       " 'bos a c h e r o r a p t o r',\n",
       " 'bos a c h i l l e s a u r u s',\n",
       " 'bos a c h i l l o b a t o r',\n",
       " 'bos a c r i s t a v u s',\n",
       " 'bos a c r o c a n t h o s a u r u s',\n",
       " 'bos a c r o t h o l u s',\n",
       " 'bos a c t i o s a u r u s',\n",
       " 'bos a d a m a n t i s a u r u s',\n",
       " 'bos a d a s a u r u s',\n",
       " 'bos a d e l o l o p h u s',\n",
       " 'bos a d e o p a p p o s a u r u s',\n",
       " 'bos a e g y p t o s a u r u s',\n",
       " 'bos a e o l o s a u r u s',\n",
       " 'bos a e p i s a u r u s',\n",
       " 'bos a e p y o r n i t h o m i m u s',\n",
       " 'bos a e r o s t e o n',\n",
       " 'bos a e t o n y x a f r o m i m u s',\n",
       " 'bos a f r o v e n a t o r',\n",
       " 'bos a g a t h a u m a s',\n",
       " 'bos a g g i o s a u r u s',\n",
       " 'bos a g i l i s a u r u s',\n",
       " 'bos a g n o s p h i t y s',\n",
       " 'bos a g r o s a u r u s',\n",
       " 'bos a g u j a c e r a t o p s',\n",
       " 'bos a g u s t i n i a',\n",
       " 'bos a h s h i s l e p e l t a',\n",
       " 'bos a i r a k o r a p t o r',\n",
       " 'bos a j a n c i n g e n i a',\n",
       " 'bos a j k a c e r a t o p s',\n",
       " 'bos a l a m o s a u r u s',\n",
       " 'bos a l a s k a c e p h a l e',\n",
       " 'bos a l b a l o p h o s a u r u s',\n",
       " 'bos a l b e r t a c e r a t o p s',\n",
       " 'bos a l b e r t a d r o m e u s',\n",
       " 'bos a l b e r t a v e n a t o r',\n",
       " 'bos a l b e r t o n y k u s',\n",
       " 'bos a l b e r t o s a u r u s',\n",
       " 'bos a l b i n y k u s',\n",
       " 'bos a l b i s a u r u s',\n",
       " 'bos a l c o v a s a u r u s',\n",
       " 'bos a l e c t r o s a u r u s',\n",
       " 'bos a l e t o p e l t a',\n",
       " 'bos a l g o a s a u r u s',\n",
       " 'bos a l i o r a m u s',\n",
       " 'bos a l i w a l i a',\n",
       " 'bos a l l o s a u r u s',\n",
       " 'bos a l m a s',\n",
       " 'bos a l n a s h e t r i',\n",
       " 'bos a l o c o d o n',\n",
       " 'bos a l t i r h i n u s',\n",
       " 'bos a l t i s p i n a x',\n",
       " 'bos a l v a r e z s a u r u s',\n",
       " 'bos a l w a l k e r i a',\n",
       " 'bos a l x a s a u r u s',\n",
       " 'bos a m a r g a s a u r u s',\n",
       " 'bos a m a r g a s t e g o s',\n",
       " 'bos a m a r g a t i t a n i s',\n",
       " 'bos a m a z o n s a u r u s',\n",
       " 'bos a m m o s a u r u s',\n",
       " 'bos a m p e l o s a u r u s',\n",
       " 'bos a m p h i c o e l i a s',\n",
       " 'bos a m p h i c o e l i c a u d i a',\n",
       " 'bos a m p h i s a u r u s',\n",
       " 'bos a m t o c e p h a l e',\n",
       " 'bos a m t o s a u r u s',\n",
       " 'bos a m u r o s a u r u s',\n",
       " 'bos a m y g d a l o d o n',\n",
       " 'bos a n a b i s e t i a',\n",
       " 'bos a n a s a z i s a u r u s',\n",
       " 'bos a n a t o s a u r u s',\n",
       " 'bos a n a t o t i t a n',\n",
       " 'bos a n c h i c e r a t o p s',\n",
       " 'bos a n c h i o r n i s',\n",
       " 'bos a n c h i s a u r u s',\n",
       " 'bos a n d e s a u r u s',\n",
       " 'bos a n d h r a s a u r u s',\n",
       " 'bos a n g a t u r a m a',\n",
       " 'bos a n g l o p o s e i d o n',\n",
       " 'bos a n g o l a t i t a n',\n",
       " 'bos a n g u l o m a s t a c a t o r',\n",
       " 'bos a n i k s o s a u r u s',\n",
       " 'bos a n i m a n t a r x',\n",
       " 'bos a n k i s t r o d o n',\n",
       " 'bos a n k y l o s a u r u s',\n",
       " 'bos a n o d o n t o s a u r u s',\n",
       " 'bos a n o p l o s a u r u s',\n",
       " 'bos a n s e r i m i m u s',\n",
       " 'bos a n t a r c t o p e l t a',\n",
       " 'bos a n t a r c t o s a u r u s',\n",
       " 'bos a n t e t o n i t r u s',\n",
       " 'bos a n t h o d o n']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_names[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b91839ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a a c h e n o s a u r u s eos',\n",
       " 'a a r d o n y x eos',\n",
       " 'a b d a l l a h s a u r u s eos',\n",
       " 'a b e l i s a u r u s eos',\n",
       " 'a b r i c t o s a u r u s eos',\n",
       " 'a b r o s a u r u s eos',\n",
       " 'a b y d o s a u r u s eos',\n",
       " 'a c a n t h o p h o l i s eos',\n",
       " 'a c h e l o u s a u r u s eos',\n",
       " 'a c h e r o r a p t o r eos',\n",
       " 'a c h i l l e s a u r u s eos',\n",
       " 'a c h i l l o b a t o r eos',\n",
       " 'a c r i s t a v u s eos',\n",
       " 'a c r o c a n t h o s a u r u s eos',\n",
       " 'a c r o t h o l u s eos',\n",
       " 'a c t i o s a u r u s eos',\n",
       " 'a d a m a n t i s a u r u s eos',\n",
       " 'a d a s a u r u s eos',\n",
       " 'a d e l o l o p h u s eos',\n",
       " 'a d e o p a p p o s a u r u s eos',\n",
       " 'a e g y p t o s a u r u s eos',\n",
       " 'a e o l o s a u r u s eos',\n",
       " 'a e p i s a u r u s eos',\n",
       " 'a e p y o r n i t h o m i m u s eos',\n",
       " 'a e r o s t e o n eos',\n",
       " 'a e t o n y x a f r o m i m u s eos',\n",
       " 'a f r o v e n a t o r eos',\n",
       " 'a g a t h a u m a s eos',\n",
       " 'a g g i o s a u r u s eos',\n",
       " 'a g i l i s a u r u s eos',\n",
       " 'a g n o s p h i t y s eos',\n",
       " 'a g r o s a u r u s eos',\n",
       " 'a g u j a c e r a t o p s eos',\n",
       " 'a g u s t i n i a eos',\n",
       " 'a h s h i s l e p e l t a eos',\n",
       " 'a i r a k o r a p t o r eos',\n",
       " 'a j a n c i n g e n i a eos',\n",
       " 'a j k a c e r a t o p s eos',\n",
       " 'a l a m o s a u r u s eos',\n",
       " 'a l a s k a c e p h a l e eos',\n",
       " 'a l b a l o p h o s a u r u s eos',\n",
       " 'a l b e r t a c e r a t o p s eos',\n",
       " 'a l b e r t a d r o m e u s eos',\n",
       " 'a l b e r t a v e n a t o r eos',\n",
       " 'a l b e r t o n y k u s eos',\n",
       " 'a l b e r t o s a u r u s eos',\n",
       " 'a l b i n y k u s eos',\n",
       " 'a l b i s a u r u s eos',\n",
       " 'a l c o v a s a u r u s eos',\n",
       " 'a l e c t r o s a u r u s eos',\n",
       " 'a l e t o p e l t a eos',\n",
       " 'a l g o a s a u r u s eos',\n",
       " 'a l i o r a m u s eos',\n",
       " 'a l i w a l i a eos',\n",
       " 'a l l o s a u r u s eos',\n",
       " 'a l m a s eos',\n",
       " 'a l n a s h e t r i eos',\n",
       " 'a l o c o d o n eos',\n",
       " 'a l t i r h i n u s eos',\n",
       " 'a l t i s p i n a x eos',\n",
       " 'a l v a r e z s a u r u s eos',\n",
       " 'a l w a l k e r i a eos',\n",
       " 'a l x a s a u r u s eos',\n",
       " 'a m a r g a s a u r u s eos',\n",
       " 'a m a r g a s t e g o s eos',\n",
       " 'a m a r g a t i t a n i s eos',\n",
       " 'a m a z o n s a u r u s eos',\n",
       " 'a m m o s a u r u s eos',\n",
       " 'a m p e l o s a u r u s eos',\n",
       " 'a m p h i c o e l i a s eos',\n",
       " 'a m p h i c o e l i c a u d i a eos',\n",
       " 'a m p h i s a u r u s eos',\n",
       " 'a m t o c e p h a l e eos',\n",
       " 'a m t o s a u r u s eos',\n",
       " 'a m u r o s a u r u s eos',\n",
       " 'a m y g d a l o d o n eos',\n",
       " 'a n a b i s e t i a eos',\n",
       " 'a n a s a z i s a u r u s eos',\n",
       " 'a n a t o s a u r u s eos',\n",
       " 'a n a t o t i t a n eos',\n",
       " 'a n c h i c e r a t o p s eos',\n",
       " 'a n c h i o r n i s eos',\n",
       " 'a n c h i s a u r u s eos',\n",
       " 'a n d e s a u r u s eos',\n",
       " 'a n d h r a s a u r u s eos',\n",
       " 'a n g a t u r a m a eos',\n",
       " 'a n g l o p o s e i d o n eos',\n",
       " 'a n g o l a t i t a n eos',\n",
       " 'a n g u l o m a s t a c a t o r eos',\n",
       " 'a n i k s o s a u r u s eos',\n",
       " 'a n i m a n t a r x eos',\n",
       " 'a n k i s t r o d o n eos',\n",
       " 'a n k y l o s a u r u s eos',\n",
       " 'a n o d o n t o s a u r u s eos',\n",
       " 'a n o p l o s a u r u s eos',\n",
       " 'a n s e r i m i m u s eos',\n",
       " 'a n t a r c t o p e l t a eos',\n",
       " 'a n t a r c t o s a u r u s eos',\n",
       " 'a n t e t o n i t r u s eos',\n",
       " 'a n t h o d o n eos']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_names[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0e9083f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bos a a c h e n o s a u r u s', 'a a c h e n o s a u r u s eos')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_names[0], Y_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95f591b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=len(alphabet)+2)\n",
    "tokenizer.fit_on_texts(X_names + Y_names)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(X_names)\n",
    "X_train = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(Y_names)\n",
    "Y_train = pad_sequences(sequences, padding='post')\n",
    "\n",
    "Y_train_cat = [to_categorical(sent, num_classes=len(alphabet) + 2) for sent in Y_train]\n",
    "Y_train = np.asarray(Y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7ee1ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  1,  1, 15, 14,  8,  6,  4,  2,  1,  3,  5,  3,  2,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1950594d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e6bd49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1536, 27), (1536, 27, 28))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baabce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos a a c h e n o s a u r u s\n",
      "a a c h e n o s a u r u s eos\n",
      "(1536, 27)\n",
      "(1536, 27, 28)\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(X_names[0])\n",
    "print(Y_names[0])\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(tokenizer.word_index['bos'])\n",
    "print(tokenizer.word_index['eos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab868f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_index = tokenizer.word_index\n",
    "index_char = {i: c for c, i in char_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5148eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(alphabet) + 2, 30, input_length=maxlen))\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "\n",
    "model.add(Dense(len(alphabet) +2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07bc8046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 460ms/step - loss: 1.6471 - accuracy: 0.5205\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 1.6351 - accuracy: 0.5205\n",
      "1/1 [==============================] - 0s 471ms/step - loss: 1.6261 - accuracy: 0.5205\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 1.6204 - accuracy: 0.5205\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 1.6175 - accuracy: 0.5205\n",
      "1/1 [==============================] - 0s 475ms/step - loss: 1.6167 - accuracy: 0.5205\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 1.6167 - accuracy: 0.5205\n",
      "1/1 [==============================] - 0s 460ms/step - loss: 1.6164 - accuracy: 0.5205\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 1.6151 - accuracy: 0.5205\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 1.6123 - accuracy: 0.5210\n",
      "1/1 [==============================] - 0s 485ms/step - loss: 1.6080 - accuracy: 0.5210\n",
      "1/1 [==============================] - 0s 464ms/step - loss: 1.6027 - accuracy: 0.5210\n",
      "1/1 [==============================] - 0s 487ms/step - loss: 1.5970 - accuracy: 0.5215\n",
      "1/1 [==============================] - 0s 480ms/step - loss: 1.5914 - accuracy: 0.5215\n",
      "1/1 [==============================] - 0s 464ms/step - loss: 1.5865 - accuracy: 0.5221\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 1.5826 - accuracy: 0.5221\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 1.5798 - accuracy: 0.5221\n",
      "1/1 [==============================] - 0s 454ms/step - loss: 1.5779 - accuracy: 0.5221\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 1.5764 - accuracy: 0.5221\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 20):\n",
    "    X_train_shuffled, y_train_shuffled = shuffle(X_train, Y_train)\n",
    "    model.fit(X_train_shuffled, y_train_shuffled, batch_size=len(X_train), epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e37826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) #/ temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.choice(range(len(alphabet)+2), p = preds)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b64f457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"bos\"\n",
      "bos \n",
      "bos m \n",
      "bos m s \n",
      "bos m s z \n",
      "bos m s z d \n",
      "bos m s z d s \n",
      "bos m s z d s b \n",
      "bos m s z d s b l \n"
     ]
    }
   ],
   "source": [
    "generated = ''\n",
    "seed = 'bos'\n",
    "generated += seed + ' '\n",
    "print('----- Generating with seed: \"' + seed + '\"')\n",
    "print(generated)\n",
    "\n",
    "for i in range(7):\n",
    "    sequences = tokenizer.texts_to_sequences([seed])\n",
    "    X_pred = pad_sequences(sequences, maxlen=maxlen, padding= 'post')\n",
    "    \n",
    "    preds = model.predict(X_pred, verbose=0)[0]\n",
    "    samples = [sample(p) for p in preds]\n",
    "    next_index = samples[i]\n",
    "    while next_index == 0 or next_index == 10:\n",
    "        samples = [sample(p) for p in preds]\n",
    "        next_index = samples[i]\n",
    "    next_char = index_char[next_index]\n",
    "    generated += next_char + ' '\n",
    "    print(generated)\n",
    "    seed += next_char\n",
    "    if next_char == 'eos':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f95d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
