{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f038b0",
   "metadata": {},
   "source": [
    "#  word2vec, doc2vec и fasttext \n",
    "\n",
    "\n",
    "1. word2vec – векторное представление слова\n",
    "2. как сделать вектор документа?\n",
    "    * усреднить все вектора слов\n",
    "    * усреднить все вектора слов с $tf-idf$ весами\n",
    "    * doc2vec\n",
    "3. fasttext – векторное представление $n$-грам\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7493cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "random.seed(1228)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91d050",
   "metadata": {},
   "source": [
    "Загружаем лемматизированные статьи без стоп-слов и создаем массив текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1718a37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "regex = re.compile(\"[А-Яа-я:=!\\)\\()A-z\\_\\%/|]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \"\".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3679655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.read_csv('datasets/nlp/negative.csv', sep=';', header=None, usecols=[3])\n",
    "df_pos = pd.read_csv('datasets/nlp/positive.csv', sep=';', header=None, usecols=[3])\n",
    "df_neg['sent'] = 'neg'\n",
    "df_pos['sent'] = 'pos'\n",
    "df = pd.concat([df_neg, df_pos])\n",
    "df = df[:1000]\n",
    "df.columns = ['text', 'sent']\n",
    "df.text = df.text.apply(words_only)\n",
    "df.text = df.text.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de929428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos[3].tolist()[:10]\n",
    "# df_neg[3].tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e97b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ed965",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [df.text.iloc[i].split() for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed615a5c",
   "metadata": {},
   "source": [
    "## Обучение модели в gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ead04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(texts, size=100, window=5, min_count=1, workers=6)\n",
    "model.save('sent_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75302755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the loaded model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7a2f4",
   "metadata": {},
   "source": [
    "Загружаем обученную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574cb481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# model.load('sent_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\":|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fca9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['хорошо', 'плохой'], negative=['хороший'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match('борщ сметана макароны пирожок консомэ кошка'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3a55e",
   "metadata": {},
   "source": [
    "Визуализация пространства слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa677f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = []\n",
    "from nltk import FreqDist\n",
    "fd = FreqDist()\n",
    "for text in texts:\n",
    "    fd.update(text)\n",
    "for i in fd.most_common(1000):\n",
    "    top_words.append(i[0])\n",
    "    \n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa0c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_vec = model[top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200198d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(random_state=0)\n",
    "top_words_tsne = tsne.fit_transform(top_words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814bcec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "p = figure(tools='pan, wheel_zoom, reset, save',\n",
    "           toolbar_location='above',\n",
    "           title='word2vec T-SNE for most common words')\n",
    "source = ColumnDataSource(data=dict(x1=top_words_tsne[:, 0],\n",
    "                                    x2=top_words_tsne[:, 1],\n",
    "                                    names=top_words\n",
    "                                   ))\n",
    "p.scatter(x='x1', y='x2', size=8, source=source)\n",
    "\n",
    "labels = LabelSet(x='x1', y='x2', text='names', y_offset=6,\n",
    "                  text_font_size='8pt', text_color='#555555',\n",
    "                  source=source, text_align='center')\n",
    "p.add_layout(labels)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1c08f",
   "metadata": {},
   "source": [
    "### Кластеризация слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dist = 1 - cosine_similarity(top_words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 100))\n",
    "ax = dendrogram(linkage_matrix, orientation='right', labels=top_words);\n",
    "\n",
    "plt.tick_params(\\\n",
    "                axis='x', # changes apply to x-axis\n",
    "                which='both', # both major and minor ticks are affected\n",
    "                bottom='off', # ticks along the bottom edge are off\n",
    "                top='off', # ticks along the top edge are off\n",
    "                labelbottom='off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('w2_clusters.png', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6929ce61",
   "metadata": {},
   "source": [
    "## Классификация текстов \n",
    "\n",
    "По мотивам http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text.tolist()\n",
    "y = df.sent.tolist()\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33)\n",
    "print(f'total train examples {len(y_train)}')\n",
    "print(f'total test examples {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionalityas all the other vectors\n",
    "        self.dim = len(w2v.popitem()[1])\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                   or [np.zeros(self.dim)], axis=0)\n",
    "            for word in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bcf3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(w2v.popitem()[1])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(x)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = datadict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                    for w in words if w in self.word2vec] or\n",
    "                   [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b44322",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", RandomForestClassifier(n_estimators=20))])\n",
    "rfc_w2v_tfidf = Pipeline([\n",
    "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", RandomForestClassifier(n_estimators=20))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a243b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_w2v.fit(X_train, y_train)\n",
    "pred = rfc_w2v.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "labels = rfc_w2v.classes_\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e203b2e",
   "metadata": {},
   "source": [
    "### paragpaph2vec aka doc2vec\n",
    "\n",
    "word2vec с дополнительной меткой id документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cfa126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13146f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_texts = [text.split() for text in X]\n",
    "idx = [str(i) for i in range(len(X))]\n",
    "\n",
    "docs = []\n",
    "for i in range(len(X)):\n",
    "    docs.append(TaggedDocument(splitted_texts[i], [idx[i]]))\n",
    "\n",
    "model = Doc2Vec(vector_size=300, window=5, min_count=5, workers=12, alpha=.025, min_alpha=.01, dm=0)\n",
    "model.build_vocab(docs)\n",
    "\n",
    "# docvec1 = model.docvecs[0]\n",
    "# docvecsyn1 = model.docvecs.doctag_syn0[0]\n",
    "# docsim1 = model.docvecs.most_similar[id1]\n",
    "\n",
    "model.train(docs, total_examples=len(docs), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082bf7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecVectorizer(object):\n",
    "    def __init__(self, d2v_model):\n",
    "        self.d2v_model = d2v_model\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([self.d2v_model.infer_vector(text.split()) for text in X])\n",
    "    \n",
    "\n",
    "rfc_d2v = Pipeline([\n",
    "    ('word2vec vectorizer', Doc2VecVectorizer(model)),\n",
    "    ('extra trees', RandomForestClassifier(n_estimators=20))])\n",
    "\n",
    "rfc_d2v.fit(X_train, y_train)\n",
    "pred = rfc_d2v.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "labels = rfc_w2v.classes_\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af1928c",
   "metadata": {},
   "source": [
    "### fast text\n",
    "\n",
    "Слово $w$ представляем символьными $n$-грамами: \n",
    "\n",
    "$n=3$, $G_{where} = \\_wh, whe, her, re\\_, \\_where\\_$\n",
    "\n",
    "$sim_{w2v}(u,v) = <u,v>$\n",
    "\n",
    "\n",
    "$sim_{ft}(u,v) = \\sum_{e \\in G_u} \\sum_{g \\in G_v} <e,v>$\n",
    "\n",
    "\n",
    "https://github.com/facebookresearch/fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61991109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "with open('datasets/nlp/data.train.txt', 'w+') as outfile:\n",
    "    for i in range(len(X_train)):\n",
    "        outfile.write('__label__' + y_train[i] + ' ' + X_train[i] +  '\\n')\n",
    "\n",
    "with open('datasets/nlp/test.txt', 'w+') as outfile:\n",
    "    for i in range(len(X_test)):\n",
    "        outfile.write('__label__' + y_test[i] + ' ' + X_test[i] +  '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873cd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fasttext.train_supervised('datasets/nlp/data.train.txt', lr=1.0, epoch=25)\n",
    "result = classifier.test('datasets/nlp/test.txt')\n",
    "\n",
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
    "\n",
    "print_results(*classifier.test('datasets/nlp/test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323980f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in name_list:\n",
    "    item = item.replace(\"\\n\",\" \")\n",
    "    pred = classifier.predict(item)\n",
    "\n",
    "\n",
    "print(\"Precision: {0:6.2f}\".format(precision_score(y_test, pred, average='macro')))\n",
    "print(\"Recall: {0:6.2f}\".format(recall_score(y_test, pred, average='macro')))\n",
    "print(\"F1-measure: {0:6.2f}\".format(f1_score(y_test, pred, average='macro')))\n",
    "print(\"Accuracy: {0:6.2f}\".format(accuracy_score(y_test, pred)))\n",
    "print(classification_report(y_test, pred))\n",
    "labels = rfc_w2v.classes_\n",
    "\n",
    "\n",
    "sns.heatmap(data=confusion_matrix(y_test, pred), annot=True, fmt=\"d\", cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "348.883px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
