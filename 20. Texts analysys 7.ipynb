{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0602dbf0",
   "metadata": {},
   "source": [
    "#  word2vec, doc2vec и fasttext \n",
    "\n",
    "\n",
    "1. word2vec – векторное представление слова\n",
    "2. как сделать вектор документа?\n",
    "    * усреднить все вектора слов\n",
    "    * усреднить все вектора слов с $tf-idf$ весами\n",
    "    * doc2vec\n",
    "3. fasttext – векторное представление $n$-грам\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98f79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "random.seed(1228)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4ef4e",
   "metadata": {},
   "source": [
    "Загружаем лемматизированные статьи без стоп-слов и создаем массив текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f297c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "\n",
    "regex = re.compile(\"[А-Яа-я:=!\\)\\()A-z\\_\\%/|]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \"\".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.read_csv('datasets/nlp/negative.csv', sep=';', header=None, usecols=[3])\n",
    "df_pos = pd.read_csv('datasets/nlp/positive.csv', sep=';', header=None, usecols=[3])\n",
    "df_neg['sent'] = 'neg'\n",
    "df_pos['sent'] = 'pos'\n",
    "df = pd.concat([df_neg, df_pos])\n",
    "df.columns = ['text', 'sent']\n",
    "df.text = df.text.apply(words_only)\n",
    "df.text = df.text.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [df.text.iloc[i].split() for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a81377",
   "metadata": {},
   "source": [
    "## Обучение модели в gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e938f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bf475",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(text, size=100, window=5, min_count=5, workers=6)\n",
    "model.save('sent_w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbc4355",
   "metadata": {},
   "source": [
    "Загружаем обученную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word\n",
    "model.load('sent_w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(':)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758039ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['хорошо', 'плохой'], negative=['хороший'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffb31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match('борщ сметана макароны пирожок консомэ кошка'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f3208f",
   "metadata": {},
   "source": [
    "Визуализация пространства слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ac60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = []\n",
    "from nltk import FreqDist\n",
    "fd = FreqDist()\n",
    "for text in texts:\n",
    "    fd.update(text)\n",
    "for i in fd.most_common(1000):\n",
    "    top_words.append(i[0])\n",
    "    \n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_vec = model[top_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_componets=2, random_state=0)\n",
    "top_words_tsne = tsne.fit_transform(top_words_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "p = figure(tools='pan, wheel_zoom, reset, save,\n",
    "           toolbar_location='above',\n",
    "           title='word2vec T-SNE for most common words')\n",
    "source = ColumnDataSource(data=dict(x1=top_word_tsne[:, 0],\n",
    "                                    x2=top_word_tsne[:, 1],\n",
    "                                    names=top_words\n",
    "                                   ))\n",
    "p.scatter(x='x1', y='x2', size=8, source=source)\n",
    "\n",
    "labels = LabelSet(x='x1', y='x2', text='names', y_offset=6,\n",
    "                  text_font_size='8pt', text_color='#555555',\n",
    "                  source=source, text_align='center')\n",
    "p.add_layout(labels)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed3d643",
   "metadata": {},
   "source": [
    "### Кластеризация слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dist = 1 - cosine_similarity(top_woprds_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34294afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 100))\n",
    "ax = dendrogram(linkage_matrix, orientation='right', labels=top_words);\n",
    "\n",
    "plt.tick_params(\\\n",
    "                axis='x', # changes apply to x-axis\n",
    "                which='both', # both major and minor ticks are affected\n",
    "                bottom='off', # ticks along the bottom edge are off\n",
    "                top='off', # ticks along the top edge are off\n",
    "                label_bottom='off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('w2_clusters.pgn', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f57ab",
   "metadata": {},
   "source": [
    "## Классификация текстов \n",
    "\n",
    "По мотивам http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text.tolist()\n",
    "y = df.sent.tolist()\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train, X_train, y_train = train_test_split(X, y, text_size=.33)\n",
    "print(f'total train examples {len(y_train)}')\n",
    "print(f'total test examples {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0320a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionalityas all the other vectors\n",
    "        self.dim = len(w2v.popitem()[1])\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                   or [np.zeros(self.dim)], axis=0)\n",
    "            for word in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(w2v.popitem()[1])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(x)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = datadict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                    for w in words if w in self.word2vec] or\n",
    "                   [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wvindex2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_w2v = Pipeline([\n",
    "    ('word2vec vectorizer', MeanEmbeddingVectorizer(w2v)),\n",
    "    ('extra trees', RandomForestClassifier(n_estimators=20))])\n",
    "\n",
    "rfc_w2v_tfidf = Pipeline([\n",
    "    ('word2vec vectorizer', TfidfEmbeddingVectorizer(w2v)),\n",
    "    ('extra trees', RandomForestClassifier(n_estimators=20))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_w2v.fit(X_train, y_train)\n",
    "pred = rfc_w2v.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312f4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "348.883px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
