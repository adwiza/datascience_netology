{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b562fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d784983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'datasets/lastfm-dataset-360K/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee60882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR + 'usersha1-artmbid-artname-plays.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d52f258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>3bd73256-3905-4f3a-97e2-8b341527f805</td>\n",
       "      <td>betty blowtorch</td>\n",
       "      <td>2137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>f2fb0ff0-5679-42ec-a55c-15109ce6e320</td>\n",
       "      <td>die Ärzte</td>\n",
       "      <td>1099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>b3ae82c2-e60b-4551-a76d-6620f1b456aa</td>\n",
       "      <td>melissa etheridge</td>\n",
       "      <td>897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>3d6bbeb7-f90e-4d10-b440-e153c0d10b53</td>\n",
       "      <td>elvenking</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000c289a1829a808ac09c00daf10bc3c4e223b</td>\n",
       "      <td>bbd2ffd7-17f4-4506-8572-c1ea58c3f9a8</td>\n",
       "      <td>juliette &amp; the licks</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0  \\\n",
       "0  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
       "1  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
       "2  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
       "3  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
       "4  00000c289a1829a808ac09c00daf10bc3c4e223b   \n",
       "\n",
       "                                      1                     2     3  \n",
       "0  3bd73256-3905-4f3a-97e2-8b341527f805       betty blowtorch  2137  \n",
       "1  f2fb0ff0-5679-42ec-a55c-15109ce6e320             die Ärzte  1099  \n",
       "2  b3ae82c2-e60b-4551-a76d-6620f1b456aa     melissa etheridge   897  \n",
       "3  3d6bbeb7-f90e-4d10-b440-e153c0d10b53             elvenking   717  \n",
       "4  bbd2ffd7-17f4-4506-8572-c1ea58c3f9a8  juliette & the licks   706  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a82f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config('spark.driver.memory', '8G')\n",
    "    .config('spark.sql.analyzer.failAmbiguousSelfJoin', 'False')\n",
    "    .master('local[*]')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a80836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark.sql.functions as sql_func\n",
    "\n",
    "plays = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(\n",
    "        os.path.join(DATA_DIR, 'usersha1-artmbid-artname-plays.tsv'),\n",
    "        header=False,\n",
    "        inferSchema=True,\n",
    "        sep='\\t'\n",
    "    )\n",
    "    .sample(withReplacement=False, fraction=1.0, seed=42)\n",
    "    # имена колонкам можно узнать из файла README.txt\n",
    "    .toDF('user', 'artist', 'artist_name', 'plays')\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "319cee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего пользователей:  359349\n",
      "Всего исполнителей:  292577\n",
      "Всего пар пльзователь - исполнитель:  17559530\n",
      "Процент ненулевых пар:  0.00016701530241348985\n"
     ]
    }
   ],
   "source": [
    "# этот набор данных гораздо более разреженный, чем movielens\n",
    "total_users = plays.select('user').distinct().count()\n",
    "total_artists = plays.select('artist_name').distinct().count()\n",
    "total_samples = plays.count()\n",
    "print('Всего пользователей: ', total_users)\n",
    "print('Всего исполнителей: ', total_artists)\n",
    "print('Всего пар пльзователь - исполнитель: ', total_samples)\n",
    "print('Процент ненулевых пар: ', total_samples / total_users / total_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b72545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             plays|\n",
      "+-------+------------------+\n",
      "|  count|          17559518|\n",
      "|   mean|215.18542764100928|\n",
      "| stddev|   614.44796397153|\n",
      "|    min|               0.0|\n",
      "|    max|          419157.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# в отличии от movielens количество прослущшиваний имеет гораздо больший размах\n",
    "plays.select('plays').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df99e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------+-----------+-----+\n",
      "|user                                    |artist|artist_name|plays|\n",
      "+----------------------------------------+------+-----------+-----+\n",
      "|cbec2d2763c856034f5689ba13ca8addb9d24984|null  |28         |0.0  |\n",
      "+----------------------------------------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# есть какие-то поользователи, совершившие меньше отдного прослушивания\n",
    "plays.where('plays < 1').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec1b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   1|\n",
      "+----+\n",
      "|null|\n",
      "|   2|\n",
      "|   3|\n",
      "|  15|\n",
      "|  35|\n",
      "|  36|\n",
      "|  37|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# смотрим на распределение длин идентификаторов\n",
    "plays.select(sql_func.length('artist').alias('1')).distinct().orderBy('1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67eb771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|         artist|   artist_name|\n",
      "+---------------+--------------+\n",
      "|            211| 000 promises.|\n",
      "|rock / a30a400a|blind the fold|\n",
      "|            313|    000 things|\n",
      "|             96|    000 things|\n",
      "+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# исполнители курильщика\n",
    "plays.where('LENGTH(artist) < 35').select('artist', 'artist_name').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d9c65a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+---------------------+\n",
      "|artist                              |artist_name          |\n",
      "+------------------------------------+---------------------+\n",
      "|8bfac288-ccc5-448d-9573-c33ea2aa5c30|red hot chili peppers|\n",
      "|1595addf-f76b-450a-a097-af852ff35f27|nujabes              |\n",
      "|7e482754-d3f6-49e5-b351-235849754e26|tristania            |\n",
      "|5bb2a9c8-3505-4c45-9717-19f0f8f35425|orgy                 |\n",
      "|ef6e2e49-aa93-41bd-89b0-8c7d2f260a83|lostprophets         |\n",
      "+------------------------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  исполнители здорового человека\n",
    "plays.where('LENGTH(artist) >= 35').select('artist', 'artist_name').distinct().show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dbe60b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|length(user)|\n",
      "+------------+\n",
      "|          12|\n",
      "|          40|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# аналогичные странности есть с ID пользователя\n",
    "plays.select(sql_func.length('user')).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b564ff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|        user|\n",
      "+------------+\n",
      "|sep 20, 2008|\n",
      "|dec 27, 2008|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# идентифиакторы длины 12 выглядят странно\n",
    "plays.where('LENGTH(user) == 12').select('user').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83c52bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|            plays|\n",
      "+-------+-----------------+\n",
      "|  count|         17332977|\n",
      "|   mean|216.0470919681022|\n",
      "| stddev| 616.996942719551|\n",
      "|    min|              1.0|\n",
      "|    max|         419157.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# будем в дальнейшем использовать очищенное множество\n",
    "pure_plays = (\n",
    "    plays\n",
    "    .where('LENGTH(artist) >= 35 AND LENGTH(user) == 40')\n",
    "    .cache()\n",
    ")\n",
    "pure_plays.select('plays').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fca66fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         ln(plays)|\n",
      "+-------+------------------+\n",
      "|  count|          17332977|\n",
      "|   mean| 4.426035251429405|\n",
      "| stddev|1.4618746810751015|\n",
      "|    min|               0.0|\n",
      "|    max| 12.94600083039178|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# для неявной оьратной связи в таком виде обычно хорошо работает\n",
    "# логарифмическая шкала\n",
    "pure_plays.select(sql_func.log('plays')).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a9c81aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='bucket'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# распределение логарифма количества прослушиваний\n",
    "(\n",
    "    pure_plays\n",
    "    .select(sql_func.floor(sql_func.log('plays')).alias('bucket'))\n",
    "    .groupBy('bucket')\n",
    "    .agg(sql_func.count('bucket').alias('cnt'))\n",
    "    .orderBy('bucket')\n",
    "    .toPandas()\n",
    "    .plot(x='bucket', y='cnt')\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68bc2a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>total_plays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b10bbbfc-cf9e-42e0-be17-e2c3e1d2600d</td>\n",
       "      <td>the beatles</td>\n",
       "      <td>30499140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a74b1b7f-71a5-4011-9441-d0b5e4122711</td>\n",
       "      <td>radiohead</td>\n",
       "      <td>27452124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cc197bad-dc9c-440d-a5b5-d52ba2e14234</td>\n",
       "      <td>coldplay</td>\n",
       "      <td>16701858.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83d91898-7763-47d7-b03b-b92132375c47</td>\n",
       "      <td>pink floyd</td>\n",
       "      <td>15965959.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65f4f0c5-ef9e-490c-aee3-909e7ae6b2ab</td>\n",
       "      <td>metallica</td>\n",
       "      <td>15498759.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 artist  artist_name  total_plays\n",
       "0  b10bbbfc-cf9e-42e0-be17-e2c3e1d2600d  the beatles   30499140.0\n",
       "1  a74b1b7f-71a5-4011-9441-d0b5e4122711    radiohead   27452124.0\n",
       "2  cc197bad-dc9c-440d-a5b5-d52ba2e14234     coldplay   16701858.0\n",
       "3  83d91898-7763-47d7-b03b-b92132375c47   pink floyd   15965959.0\n",
       "4  65f4f0c5-ef9e-490c-aee3-909e7ae6b2ab    metallica   15498759.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выберем самых популярных испольнителей\n",
    "top_artists = (\n",
    "    pure_plays\n",
    "    .groupBy('artist', 'artist_name')\n",
    "    .agg(sql_func.sum('plays').alias('total_plays'))\n",
    "    .orderBy(sql_func.desc('total_plays'))\n",
    "    .limit(100)\n",
    "    .toPandas()\n",
    ")\n",
    "top_artists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "119c19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посчитаем hitrate@N\n",
    "def hitrate_at_n(n: int) -> float:\n",
    "    return (\n",
    "        pure_plays\n",
    "        .join(\n",
    "            sql_func.broadcast(spark.createDataFrame(top_artists[:n])),\n",
    "            on='artist',\n",
    "            how='left'\n",
    "        )\n",
    "        .groupBy('user')\n",
    "        .agg(sql_func.max('total_plays').alias('some_plays'))\n",
    "        .select(\n",
    "            sql_func.col('user'),\n",
    "            sql_func.when(sql_func.col('some_plays').isNotNull(), 1).otherwise(0).alias('hit')\n",
    "        )\n",
    "        .agg(sql_func.avg('hit').alias('hit_rate'))\n",
    "        .first()[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b26be233",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o299.collectToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition, ENSURE_REQUIREMENTS, [id=#533]\n+- *(3) HashAggregate(keys=[], functions=[partial_avg(cast(hit#1710 as bigint))], output=[sum#1780, count#1781L])\n   +- *(3) HashAggregate(keys=[user#24], functions=[max(total_plays#1690)], output=[hit#1710])\n      +- Exchange hashpartitioning(user#24, 200), ENSURE_REQUIREMENTS, [id=#528]\n         +- *(2) HashAggregate(keys=[user#24], functions=[partial_max(total_plays#1690)], output=[user#24, max#1783])\n            +- *(2) Project [user#24, total_plays#1690]\n               +- *(2) BroadcastHashJoin [artist#25], [artist#1688], LeftOuter, BuildRight, false\n                  :- InMemoryTableScan [user#24, artist#25]\n                  :     +- InMemoryRelation [user#24, artist#25, artist_name#26, plays#27], StorageLevel(disk, memory, deserialized, 1 replicas)\n                  :           +- *(1) Filter (((isnotnull(artist#25) AND isnotnull(user#24)) AND (length(artist#25) >= 35)) AND (length(user#24) = 40))\n                  :              +- InMemoryTableScan [user#24, artist#25, artist_name#26, plays#27], [isnotnull(artist#25), isnotnull(user#24), (length(artist#25) >= 35), (length(user#24) = 40)]\n                  :                    +- InMemoryRelation [user#24, artist#25, artist_name#26, plays#27], StorageLevel(disk, memory, deserialized, 1 replicas)\n                  :                          +- *(1) Project [_c0#16 AS user#24, _c1#17 AS artist#25, _c2#18 AS artist_name#26, _c3#19 AS plays#27]\n                  :                             +- *(1) Sample 0.0, 1.0, false, 42\n                  :                                +- FileScan csv [_c0#16,_c1#17,_c2#18,_c3#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/adwiz/Documents/Courses/datascience_netology/datasets/lastfm-dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:double>\n                  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#522]\n                     +- *(1) Project [artist#1688, total_plays#1690]\n                        +- *(1) Filter isnotnull(artist#1688)\n                           +- *(1) Scan ExistingRDD[artist#1688,artist_name#1689,total_plays#1690]\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:387)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(user#24, 200), ENSURE_REQUIREMENTS, [id=#528]\n+- *(2) HashAggregate(keys=[user#24], functions=[partial_max(total_plays#1690)], output=[user#24, max#1783])\n   +- *(2) Project [user#24, total_plays#1690]\n      +- *(2) BroadcastHashJoin [artist#25], [artist#1688], LeftOuter, BuildRight, false\n         :- InMemoryTableScan [user#24, artist#25]\n         :     +- InMemoryRelation [user#24, artist#25, artist_name#26, plays#27], StorageLevel(disk, memory, deserialized, 1 replicas)\n         :           +- *(1) Filter (((isnotnull(artist#25) AND isnotnull(user#24)) AND (length(artist#25) >= 35)) AND (length(user#24) = 40))\n         :              +- InMemoryTableScan [user#24, artist#25, artist_name#26, plays#27], [isnotnull(artist#25), isnotnull(user#24), (length(artist#25) >= 35), (length(user#24) = 40)]\n         :                    +- InMemoryRelation [user#24, artist#25, artist_name#26, plays#27], StorageLevel(disk, memory, deserialized, 1 replicas)\n         :                          +- *(1) Project [_c0#16 AS user#24, _c1#17 AS artist#25, _c2#18 AS artist_name#26, _c3#19 AS plays#27]\n         :                             +- *(1) Sample 0.0, 1.0, false, 42\n         :                                +- FileScan csv [_c0#16,_c1#17,_c2#18,_c3#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/adwiz/Documents/Courses/datascience_netology/datasets/lastfm-dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:double>\n         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#522]\n            +- *(1) Project [artist#1688, total_plays#1690]\n               +- *(1) Filter isnotnull(artist#1688)\n                  +- *(1) Scan ExistingRDD[artist#1688,artist_name#1689,total_plays#1690]\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 39 more\r\nCaused by: java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 61.0 failed 1 times, most recent failure: Lost task 3.0 in stage 61.0 (TID 2032) (192.168.3.7 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 473, in main\nException: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n\nDriver stacktrace:\r\n\tat java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n\tat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:194)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:515)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:203)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareRelation(BroadcastHashJoinExec.scala:217)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter(HashJoin.scala:503)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter$(HashJoin.scala:502)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:40)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:358)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:355)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\r\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:352)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:351)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 63 more\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 61.0 failed 1 times, most recent failure: Lost task 3.0 in stage 61.0 (TID 2032) (192.168.3.7 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 473, in main\nException: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:397)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:185)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 473, in main\nException: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\t... 3 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-0f89b28748bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# чем больше элементов мы можем порекомфендовать, тем лучше метрики\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hitrate{} = {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhitrate_at_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-1472153fce8b>\u001b[0m in \u001b[0;36mhitrate_at_n\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhitrate_at_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     return (\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mpure_plays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         .join(\n\u001b[0;32m      6\u001b[0m             \u001b[0msql_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_artists\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1599\u001b[0m         \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m         \"\"\"\n\u001b[1;32m-> 1601\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1585\u001b[0m         \"\"\"\n\u001b[0;32m   1586\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1587\u001b[1;33m             \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1588\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1587\u001b[0m             \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1589\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1591\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \"\"\"\n\u001b[1;32m--> 728\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \"\"\"\n\u001b[0;32m    676\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o299.collectToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition, ENSURE_REQUIREMENTS, [id=#533]\n+- *(3) HashAggregate(keys=[], functions=[partial_avg(cast(hit#1710 as bigint))], output=[sum#1780, count#1781L])\n   +- *(3) HashAggregate(keys=[user#24], functions=[max(total_plays#1690)], output=[hit#1710])\n      +- Exchange hashpartitioning(user#24, 200), ENSURE_REQUIREMENTS, [id=#528]\n         +- *(2) HashAggregate(keys=[user#24], functions=[partial_max(total_plays#1690)], output=[user#24, max#1783])\n            +- *(2) Project [user#24, total_plays#1690]\n               +- *(2) BroadcastHashJoin [artist#25], [artist#1688], LeftOuter, BuildRight, false\n                  :- InMemoryTableScan [user#24, artist#25]\n                  :     +- InMemoryRelation [user#24, artist#25, artist_name#26, plays#27], StorageLevel(disk, memory, deserialized, 1 replicas)\n                  :           +- *(1) Filter (((isnotnull(artist#25) AND isnotnull(user#24)) AND (length(artist#25) >= 35)) AND (length(user#24) = 40))\n                  :              +- InMemoryTableScan [user#24, artist#25, artist_name#26, plays#27], [isnotnull(artist#25), isnotnull(user#24), (length(artist#25) >= 35), (length(user#24) = 40)]\n                  :                    +- InMemoryRelation [user#24, artist#25, artist_name#26, plays#27], StorageLevel(disk, memory, deserialized, 1 replicas)\n                  :                          +- *(1) Project [_c0#16 AS user#24, _c1#17 AS artist#25, _c2#18 AS artist_name#26, _c3#19 AS plays#27]\n                  :                             +- *(1) Sample 0.0, 1.0, false, 42\n                  :                                +- FileScan csv [_c0#16,_c1#17,_c2#18,_c3#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/adwiz/Documents/Courses/datascience_netology/datasets/lastfm-dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:double>\n                  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#522]\n                     +- *(1) Project [artist#1688, total_plays#1690]\n                        +- *(1) Filter isnotnull(artist#1688)\n                           +- *(1) Scan ExistingRDD[artist#1688,artist_name#1689,total_plays#1690]\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:387)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(user#24, 200), ENSURE_REQUIREMENTS, [id=#528]\n+- *(2) HashAggregate(keys=[user#24], functions=[partial_max(total_plays#1690)], output=[user#24, max#1783])\n   +- *(2) Project [user#24, total_plays#1690]\n      +- *(2) BroadcastHashJoin [artist#25], [artist#1688], LeftOuter, BuildRight, false\n         :- InMemoryTableScan [user#24, artist#25]\n         :     +- InMemoryRelation [user#24, artist#25, artist_name#26, plays#27], StorageLevel(disk, memory, deserialized, 1 replicas)\n         :           +- *(1) Filter (((isnotnull(artist#25) AND isnotnull(user#24)) AND (length(artist#25) >= 35)) AND (length(user#24) = 40))\n         :              +- InMemoryTableScan [user#24, artist#25, artist_name#26, plays#27], [isnotnull(artist#25), isnotnull(user#24), (length(artist#25) >= 35), (length(user#24) = 40)]\n         :                    +- InMemoryRelation [user#24, artist#25, artist_name#26, plays#27], StorageLevel(disk, memory, deserialized, 1 replicas)\n         :                          +- *(1) Project [_c0#16 AS user#24, _c1#17 AS artist#25, _c2#18 AS artist_name#26, _c3#19 AS plays#27]\n         :                             +- *(1) Sample 0.0, 1.0, false, 42\n         :                                +- FileScan csv [_c0#16,_c1#17,_c2#18,_c3#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/adwiz/Documents/Courses/datascience_netology/datasets/lastfm-dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:double>\n         +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#522]\n            +- *(1) Project [artist#1688, total_plays#1690]\n               +- *(1) Filter isnotnull(artist#1688)\n                  +- *(1) Scan ExistingRDD[artist#1688,artist_name#1689,total_plays#1690]\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 39 more\r\nCaused by: java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 61.0 failed 1 times, most recent failure: Lost task 3.0 in stage 61.0 (TID 2032) (192.168.3.7 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 473, in main\nException: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n\nDriver stacktrace:\r\n\tat java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n\tat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:194)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:515)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:203)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareRelation(BroadcastHashJoinExec.scala:217)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter(HashJoin.scala:503)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.codegenOuter$(HashJoin.scala:502)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:40)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:358)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:355)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\r\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:352)\r\n\tat org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:351)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 63 more\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 61.0 failed 1 times, most recent failure: Lost task 3.0 in stage 61.0 (TID 2032) (192.168.3.7 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 473, in main\nException: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:397)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:185)\r\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 473, in main\nException: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\t... 3 more\r\n"
     ]
    }
   ],
   "source": [
    "# чем больше элементов мы можем порекомфендовать, тем лучше метрики\n",
    "for n in [100, 50, 20, 10, 5, 1]:\n",
    "    print('hitrate{} = {}'.format(n, hitrate_at_n(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d4737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
